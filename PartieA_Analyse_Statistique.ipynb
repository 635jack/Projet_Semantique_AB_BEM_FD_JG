{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Partie A : Analyse Statistique du Corpus NER\n",
                "\n",
                "**Universit√© Paris Cit√© - Master 2 VMI**  \n",
                "**Projet Knowledge Extraction**  \n",
                "**Auteur :** Jacques Gastebois\n",
                "\n",
                "---\n",
                "\n",
                "## Objectifs\n",
                "\n",
                "Cette analyse vise √† :\n",
                "1. **Caract√©riser le corpus** : statistiques descriptives, distributions\n",
                "2. **Visualiser les donn√©es** : graphiques professionnels\n",
                "3. **Analyser les entit√©s NER** : r√©partition des types d'entit√©s\n",
                "4. **Analyser l'impact du preprocessing** : avant/apr√®s lemmatization\n",
                "5. **Connecter avec les autres parties** : lien avec PartII (NER) et PartieC (relations)\n",
                "6. **Tirer des conclusions** : insights pour la suite du projet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup et Chargement des Donn√©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "!{sys.executable} -m pip install -q pandas numpy matplotlib seaborn wordcloud\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from wordcloud import WordCloud\n",
                "import json\n",
                "import ast\n",
                "from collections import Counter\n",
                "\n",
                "# Configuration des graphiques\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 10\n",
                "\n",
                "print(\"‚úÖ Setup termin√©\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement du corpus pr√©trait√©\n",
                "df = pd.read_csv('data_preprocessed.csv')\n",
                "\n",
                "print(f\"üìä Corpus charg√© : {len(df)} phrases\")\n",
                "print(f\"\\nColonnes disponibles :\")\n",
                "for col in df.columns:\n",
                "    print(f\"  - {col}\")\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Statistiques Descriptives G√©n√©rales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calcul des longueurs de texte\n",
                "df['text_length'] = df['text'].str.len()\n",
                "df['cleaned_length'] = df['cleaned_text'].str.len()\n",
                "df['lemmatized_length'] = df['lemmatized_text'].str.len()\n",
                "\n",
                "# Nombre de mots\n",
                "df['num_words'] = df['text'].str.split().str.len()\n",
                "df['num_words_lemmatized'] = df['lemmatized_text'].str.split().str.len()\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STATISTIQUES G√âN√âRALES DU CORPUS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüìè Longueur des textes (caract√®res) :\")\n",
                "print(f\"  Moyenne : {df['text_length'].mean():.1f}\")\n",
                "print(f\"  M√©diane : {df['text_length'].median():.1f}\")\n",
                "print(f\"  Min     : {df['text_length'].min()}\")\n",
                "print(f\"  Max     : {df['text_length'].max()}\")\n",
                "print(f\"  √âcart-type : {df['text_length'].std():.1f}\")\n",
                "\n",
                "print(f\"\\nüìù Nombre de mots par phrase :\")\n",
                "print(f\"  Moyenne : {df['num_words'].mean():.1f}\")\n",
                "print(f\"  M√©diane : {df['num_words'].median():.1f}\")\n",
                "print(f\"  Min     : {df['num_words'].min()}\")\n",
                "print(f\"  Max     : {df['num_words'].max()}\")\n",
                "\n",
                "print(f\"\\nüîÑ Impact du preprocessing :\")\n",
                "reduction = (1 - df['cleaned_length'].mean() / df['text_length'].mean()) * 100\n",
                "print(f\"  R√©duction (nettoyage) : {reduction:.1f}%\")\n",
                "reduction_lem = (1 - df['lemmatized_length'].mean() / df['cleaned_length'].mean()) * 100\n",
                "print(f\"  R√©duction (lemmatization) : {reduction_lem:.1f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualisations : Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution des longueurs de phrases\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Histogramme\n",
                "axes[0].hist(df['num_words'], bins=50, edgecolor='black', alpha=0.7)\n",
                "axes[0].axvline(df['num_words'].mean(), color='red', linestyle='--', \n",
                "                linewidth=2, label=f'Moyenne: {df[\"num_words\"].mean():.1f}')\n",
                "axes[0].axvline(df['num_words'].median(), color='green', linestyle='--', \n",
                "                linewidth=2, label=f'M√©diane: {df[\"num_words\"].median():.1f}')\n",
                "axes[0].set_xlabel('Nombre de mots par phrase')\n",
                "axes[0].set_ylabel('Fr√©quence')\n",
                "axes[0].set_title('Distribution du Nombre de Mots par Phrase')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Boxplot\n",
                "axes[1].boxplot([df['num_words']], labels=['Corpus'])\n",
                "axes[1].set_ylabel('Nombre de mots')\n",
                "axes[1].set_title('Boxplot : Nombre de Mots par Phrase')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('stats_distributions.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Graphique sauvegard√© : stats_distributions.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comparaison avant/apr√®s preprocessing\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "data_to_plot = [\n",
                "    df['text_length'].values,\n",
                "    df['cleaned_length'].values,\n",
                "    df['lemmatized_length'].values\n",
                "]\n",
                "\n",
                "bp = ax.boxplot(data_to_plot, \n",
                "                labels=['Texte Original', 'Texte Nettoy√©', 'Texte Lemmatis√©'],\n",
                "                patch_artist=True,\n",
                "                showmeans=True)\n",
                "\n",
                "# Couleurs\n",
                "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "\n",
                "ax.set_ylabel('Longueur (caract√®res)')\n",
                "ax.set_title('Impact du Preprocessing sur la Longueur des Textes')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('stats_preprocessing_impact.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Graphique sauvegard√© : stats_preprocessing_impact.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyse du Vocabulaire"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extraction du vocabulaire\n",
                "all_words_original = ' '.join(df['text'].fillna('')).lower().split()\n",
                "all_words_lemmatized = ' '.join(df['lemmatized_text'].fillna('')).split()\n",
                "\n",
                "vocab_original = set(all_words_original)\n",
                "vocab_lemmatized = set(all_words_lemmatized)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ANALYSE DU VOCABULAIRE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüìö Taille du vocabulaire :\")\n",
                "print(f\"  Original     : {len(vocab_original):,} mots uniques\")\n",
                "print(f\"  Lemmatis√©    : {len(vocab_lemmatized):,} mots uniques\")\n",
                "print(f\"  R√©duction    : {(1 - len(vocab_lemmatized)/len(vocab_original))*100:.1f}%\")\n",
                "\n",
                "print(f\"\\nüî¢ Tokens totaux :\")\n",
                "print(f\"  Original     : {len(all_words_original):,} tokens\")\n",
                "print(f\"  Lemmatis√©    : {len(all_words_lemmatized):,} tokens\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top 20 mots les plus fr√©quents\n",
                "word_freq_original = Counter(all_words_original)\n",
                "word_freq_lemmatized = Counter(all_words_lemmatized)\n",
                "\n",
                "top_20_original = word_freq_original.most_common(20)\n",
                "top_20_lemmatized = word_freq_lemmatized.most_common(20)\n",
                "\n",
                "# Visualisation\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Top mots originaux\n",
                "words_orig, counts_orig = zip(*top_20_original)\n",
                "axes[0].barh(range(len(words_orig)), counts_orig, color='steelblue')\n",
                "axes[0].set_yticks(range(len(words_orig)))\n",
                "axes[0].set_yticklabels(words_orig)\n",
                "axes[0].invert_yaxis()\n",
                "axes[0].set_xlabel('Fr√©quence')\n",
                "axes[0].set_title('Top 20 Mots - Texte Original (lowercase)')\n",
                "axes[0].grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Top mots lemmatis√©s\n",
                "words_lem, counts_lem = zip(*top_20_lemmatized)\n",
                "axes[1].barh(range(len(words_lem)), counts_lem, color='coral')\n",
                "axes[1].set_yticks(range(len(words_lem)))\n",
                "axes[1].set_yticklabels(words_lem)\n",
                "axes[1].invert_yaxis()\n",
                "axes[1].set_xlabel('Fr√©quence')\n",
                "axes[1].set_title('Top 20 Mots - Texte Lemmatis√©')\n",
                "axes[1].grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('stats_top_words.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Graphique sauvegard√© : stats_top_words.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word Cloud du corpus lemmatis√©\n",
                "text_for_cloud = ' '.join(df['lemmatized_text'].fillna(''))\n",
                "\n",
                "wordcloud = WordCloud(width=1200, height=600, \n",
                "                      background_color='white',\n",
                "                      colormap='viridis',\n",
                "                      max_words=200).generate(text_for_cloud)\n",
                "\n",
                "plt.figure(figsize=(14, 7))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud - Corpus Lemmatis√© (Top 200 mots)', fontsize=16, pad=20)\n",
                "plt.tight_layout()\n",
                "plt.savefig('stats_wordcloud.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Graphique sauvegard√© : stats_wordcloud.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyse des Entit√©s NER"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mapping des tags NER\n",
                "ner_mapping = {\n",
                "    0: 'O (Other)',\n",
                "    1: 'B-LOC (Location)',\n",
                "    2: 'B-PER (Person)',\n",
                "    4: 'B-ORG (Organization)'\n",
                "}\n",
                "\n",
                "# Extraction et comptage des tags\n",
                "def count_ner_tags(ner_tags_str):\n",
                "    try:\n",
                "        tags = ast.literal_eval(ner_tags_str)\n",
                "        return Counter(tags)\n",
                "    except:\n",
                "        return Counter()\n",
                "\n",
                "all_ner_counts = Counter()\n",
                "for tags_str in df['ner_tags']:\n",
                "    all_ner_counts.update(count_ner_tags(tags_str))\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ANALYSE DES ENTIT√âS NER\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüè∑Ô∏è R√©partition des tags NER :\")\n",
                "total_tags = sum(all_ner_counts.values())\n",
                "for tag, count in sorted(all_ner_counts.items()):\n",
                "    tag_name = ner_mapping.get(tag, f'Unknown ({tag})')\n",
                "    percentage = (count / total_tags) * 100\n",
                "    print(f\"  {tag_name:25s} : {count:6,} ({percentage:5.2f}%)\")\n",
                "print(f\"\\n  Total : {total_tags:,} tags\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualisation de la r√©partition des entit√©s\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Pie chart\n",
                "labels = [ner_mapping.get(tag, f'Tag {tag}') for tag in all_ner_counts.keys()]\n",
                "sizes = list(all_ner_counts.values())\n",
                "colors = ['lightgray', 'lightblue', 'lightcoral', 'lightgreen']\n",
                "\n",
                "axes[0].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
                "axes[0].set_title('R√©partition des Tags NER')\n",
                "\n",
                "# Bar chart (sans 'O')\n",
                "entity_tags = {k: v for k, v in all_ner_counts.items() if k != 0}\n",
                "entity_labels = [ner_mapping.get(tag, f'Tag {tag}') for tag in entity_tags.keys()]\n",
                "entity_counts = list(entity_tags.values())\n",
                "\n",
                "axes[1].bar(entity_labels, entity_counts, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
                "axes[1].set_ylabel('Nombre d\\'occurrences')\n",
                "axes[1].set_title('Distribution des Entit√©s Nomm√©es (sans O)')\n",
                "axes[1].tick_params(axis='x', rotation=15)\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('stats_ner_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Graphique sauvegard√© : stats_ner_distribution.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Connexion avec les Autres Parties du Projet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyse de la Partie C : extraction de relations\n",
                "import os\n",
                "\n",
                "if os.path.exists('PartieC/extracted_triplets.json'):\n",
                "    with open('PartieC/extracted_triplets.json', 'r') as f:\n",
                "        triplets = json.load(f)\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    print(\"CONNEXION AVEC PARTIE C (EXTRACTION DE RELATIONS)\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"\\nüîó Triplets extraits : {len(triplets)}\")\n",
                "    \n",
                "    # Analyse des types de relations si disponible\n",
                "    if triplets and isinstance(triplets, list) and len(triplets) > 0:\n",
                "        print(f\"\\nExemple de triplet :\")\n",
                "        print(f\"  {triplets[0]}\")\n",
                "        \n",
                "        # Calcul du taux de couverture\n",
                "        coverage = (len(triplets) / len(df)) * 100\n",
                "        print(f\"\\nüìä Taux de couverture :\")\n",
                "        print(f\"  {coverage:.1f}% des phrases ont des relations extraites\")\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Fichier PartieC/extracted_triplets.json non trouv√©\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusions et Insights\n",
                "\n",
                "### üéØ Caract√©ristiques du Corpus\n",
                "\n",
                "**Nature du corpus :**\n",
                "- Dataset de **Named Entity Recognition** (NER) avec 2221 phrases\n",
                "- Textes courts (moyenne ~20 mots/phrase)\n",
                "- Domaine : textes encyclop√©diques/biographiques\n",
                "\n",
                "**Distribution des entit√©s :**\n",
                "- Majorit√© de tokens 'O' (Other) : texte non annot√©\n",
                "- Entit√©s principales : Personnes (B-PER), Lieux (B-LOC), Organisations (B-ORG)\n",
                "- √âquilibre entre les diff√©rents types d'entit√©s\n",
                "\n",
                "**Impact du preprocessing :**\n",
                "- **Nettoyage** : r√©duction des caract√®res sp√©ciaux (~10-15%)\n",
                "- **Lemmatization** : r√©duction du vocabulaire (~20-30%)\n",
                "- Permet une meilleure g√©n√©ralisation pour les mod√®les\n",
                "\n",
                "### üîó Connexion avec les Autres Parties\n",
                "\n",
                "**Partie II (NER) :**\n",
                "- Utilise les textes lemmatis√©s pour am√©liorer la reconnaissance d'entit√©s\n",
                "- Le preprocessing facilite la d√©tection des patterns\n",
                "- Vocabulaire r√©duit = mod√®le plus efficace\n",
                "\n",
                "**Partie C (Extraction de Relations) :**\n",
                "- S'appuie sur les entit√©s identifi√©es en Partie II\n",
                "- Extraction de triplets (sujet-relation-objet)\n",
                "- Le preprocessing pr√©alable am√©liore la qualit√© des relations extraites\n",
                "\n",
                "### üí° Recommandations pour la Suite\n",
                "\n",
                "1. **Pour la Partie B (NER) :**\n",
                "   - Utiliser les textes lemmatis√©s pour l'entra√Ænement\n",
                "   - Attention aux entit√©s rares (balance dataset)\n",
                "   - Consid√©rer les bigrammes pour les entit√©s compos√©es\n",
                "\n",
                "2. **Pour la Partie C (Relations) :**\n",
                "   - Exploiter la structure syntaxique pr√©serv√©e\n",
                "   - Focus sur les phrases avec plusieurs entit√©s\n",
                "   - Validation crois√©e avec les tags NER originaux\n",
                "\n",
                "3. **Am√©liorations possibles :**\n",
                "   - Stemming vs Lemmatization : comparaison\n",
                "   - Filtrage des stop words pour certaines t√¢ches\n",
                "   - Enrichissement avec POS tags\n",
                "\n",
                "---\n",
                "\n",
                "**üìå Conclusion g√©n√©rale :**\n",
                "\n",
                "Le preprocessing appliqu√© (nettoyage + lemmatization) constitue une **base solide** pour les t√¢ches suivantes. La r√©duction du vocabulaire et la normalisation des formes verbales facilitent l'apprentissage des mod√®les tout en pr√©servant l'information s√©mantique essentielle pour la reconnaissance d'entit√©s et l'extraction de relations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export du Rapport Statistique"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cr√©ation d'un r√©sum√© statistique en JSON\n",
                "stats_summary = {\n",
                "    'corpus': {\n",
                "        'total_sentences': len(df),\n",
                "        'avg_words_per_sentence': float(df['num_words'].mean()),\n",
                "        'avg_chars_per_sentence': float(df['text_length'].mean()),\n",
                "        'min_words': int(df['num_words'].min()),\n",
                "        'max_words': int(df['num_words'].max())\n",
                "    },\n",
                "    'vocabulary': {\n",
                "        'original_vocab_size': len(vocab_original),\n",
                "        'lemmatized_vocab_size': len(vocab_lemmatized),\n",
                "        'reduction_percentage': float((1 - len(vocab_lemmatized)/len(vocab_original))*100)\n",
                "    },\n",
                "    'ner_distribution': {ner_mapping.get(k, f'Tag_{k}'): int(v) \n",
                "                        for k, v in all_ner_counts.items()},\n",
                "    'preprocessing_impact': {\n",
                "        'cleaning_reduction': float((1 - df['cleaned_length'].mean() / df['text_length'].mean()) * 100),\n",
                "        'lemmatization_reduction': float((1 - df['lemmatized_length'].mean() / df['cleaned_length'].mean()) * 100)\n",
                "    }\n",
                "}\n",
                "\n",
                "with open('corpus_statistics.json', 'w') as f:\n",
                "    json.dump(stats_summary, f, indent=2)\n",
                "\n",
                "print(\"‚úÖ R√©sum√© statistique sauvegard√© : corpus_statistics.json\")\n",
                "print(\"\\nüìä Graphiques g√©n√©r√©s :\")\n",
                "print(\"  - stats_distributions.png\")\n",
                "print(\"  - stats_preprocessing_impact.png\")\n",
                "print(\"  - stats_top_words.png\")\n",
                "print(\"  - stats_wordcloud.png\")\n",
                "print(\"  - stats_ner_distribution.png\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
