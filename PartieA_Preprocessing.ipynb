{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Représentation Text\n",
                "\n",
                "**Université Paris Cité - Master 2 VMI**\n",
                "**Cours :** IFLCE085 Recherche et extraction sémantique à partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**Équipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## Étape 1 : Setup et Importations\n",
                "Objectif : Configurer l'environnement et importer les librairies nécessaires."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
                        "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
                        "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
                        "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
                        "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
                        "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
                        "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
                        "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
                        "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
                        "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
                        "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
                        "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
                        "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
                        "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
                        "Collecting en-core-web-sm==3.8.0\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n",
                        "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
                        "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
                        "order to load all the package's dependencies. You can do this by selecting the\n",
                        "'Restart kernel' or 'Restart runtime' option.\n",
                        "Environnement configuré avec succès.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "# Installation des dépendances de base\n",
                "!{sys.executable} -m pip install pandas\n",
                "!{sys.executable} -m pip install numpy\n",
                "!{sys.executable} -m pip install nltk\n",
                "!{sys.executable} -m pip install scikit-learn\n",
                "!{sys.executable} -m pip install spacy\n",
                "\n",
                "# Téléchargement du modèle spaCy anglais\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "# Téléchargement des ressources NLTK (version mise à jour)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du modèle spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"Environnement configuré avec succès.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 2 : Chargement et Exploration des Données (SciREX)\n",
                "Objectif : Télécharger (si nécessaire) et charger le dataset SciREX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Les données sont déjà présentes.\n",
                        "Chargement des données...\n",
                        "Nombre de documents Train : 306\n",
                        "Nombre de documents Dev   : 66\n",
                        "Nombre de documents Test  : 66\n"
                    ]
                }
            ],
            "source": [
                "import urllib.request\n",
                "import tarfile\n",
                "\n",
                "DATA_DIR = \"release_data\"\n",
                "DATA_URL = \"https://github.com/allenai/SciREX/raw/master/scirex_dataset/release_data.tar.gz\"\n",
                "TAR_FILE = \"release_data.tar.gz\"\n",
                "\n",
                "def download_and_extract_data():\n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(\"Dossier de données non trouvé. Téléchargement en cours...\")\n",
                "        try:\n",
                "            urllib.request.urlretrieve(DATA_URL, TAR_FILE)\n",
                "            print(\"Téléchargement terminé. Extraction...\")\n",
                "            with tarfile.open(TAR_FILE, \"r:gz\") as tar:\n",
                "                tar.extractall()\n",
                "            print(\"Extraction terminée.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Erreur lors du téléchargement/extraction : {e}\")\n",
                "    else:\n",
                "        print(\"Les données sont déjà présentes.\")\n",
                "\n",
                "download_and_extract_data()\n",
                "\n",
                "FILES = {\n",
                "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
                "    \"dev\": os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
                "    \"test\": os.path.join(DATA_DIR, \"test.jsonl\")\n",
                "}\n",
                "\n",
                "def load_jsonl(file_path):\n",
                "    \"\"\"Charge un fichier JSONL dans une liste de dictionnaires.\"\"\"\n",
                "    data = []\n",
                "    if os.path.exists(file_path):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                data.append(json.loads(line))\n",
                "    else:\n",
                "        print(f\"Attention : Fichier {file_path} introuvable.\")\n",
                "    return data\n",
                "\n",
                "# Chargement des données\n",
                "print(\"Chargement des données...\")\n",
                "train_data = load_jsonl(FILES[\"train\"])\n",
                "dev_data = load_jsonl(FILES[\"dev\"])\n",
                "test_data = load_jsonl(FILES[\"test\"])\n",
                "\n",
                "print(f\"Nombre de documents Train : {len(train_data)}\")\n",
                "print(f\"Nombre de documents Dev   : {len(dev_data)}\")\n",
                "print(f\"Nombre de documents Test  : {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Clés disponibles dans un document :\n",
                        "['coref', 'coref_non_salient', 'doc_id', 'method_subrelations', 'n_ary_relations', 'ner', 'sections', 'sentences', 'words']\n",
                        "\n",
                        "Exemple de contenu (champs principaux) :\n",
                        "ID: 000f90380d768a85e2316225854fc377c079b5c4\n",
                        "Début du texte (50 premiers mots) : Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an accurate understanding of the surrounding scene is crucial to navigation and action planning . Current state - of - the -...\n"
                    ]
                }
            ],
            "source": [
                "# Exploration d'un document type\n",
                "if train_data:\n",
                "    doc_example = train_data[0]\n",
                "    print(\"\\nClés disponibles dans un document :\")\n",
                "    print(list(doc_example.keys()))\n",
                "\n",
                "    print(\"\\nExemple de contenu (champs principaux) :\")\n",
                "    print(f\"ID: {doc_example.get('doc_id')}\")\n",
                "    if 'words' in doc_example:\n",
                "        print(f\"Début du texte (50 premiers mots) : {' '.join(doc_example['words'][:50])}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 3 : Nettoyage et Normalisation\n",
                "Objectif : Nettoyer le texte (lowercase, suppression caractères spéciaux, espaces multiples)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonction clean_text() définie.\n"
                    ]
                }
            ],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caractères spéciaux, espaces multiples.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à nettoyer\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte nettoyé\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caractères spéciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "print(\"Fonction clean_text() définie.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte original (200 premiers caractères) :\n",
                        "Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an acc\n",
                        "\n",
                        "Texte nettoyé (200 premiers caractères) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate \n",
                        "\n",
                        "Longueur originale : 36348 caractères\n",
                        "Longueur nettoyée  : 34301 caractères\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'words' in doc_example:\n",
                "    raw_text = ' '.join(doc_example['words'])\n",
                "    cleaned_text = clean_text(raw_text)\n",
                "    \n",
                "    print(\"Texte original (200 premiers caractères) :\")\n",
                "    print(raw_text[:200])\n",
                "    print(\"\\nTexte nettoyé (200 premiers caractères) :\")\n",
                "    print(cleaned_text[:200])\n",
                "    print(f\"\\nLongueur originale : {len(raw_text)} caractères\")\n",
                "    print(f\"Longueur nettoyée  : {len(cleaned_text)} caractères\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application du nettoyage sur tous les documents...\n",
                        "Nettoyage terminé pour 306 docs train, 66 docs dev, 66 docs test.\n",
                        "\n",
                        "Exemple de texte nettoyé (doc 0) : full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of...\n"
                    ]
                }
            ],
            "source": [
                "# Application du nettoyage sur tous les documents\n",
                "print(\"Application du nettoyage sur tous les documents...\")\n",
                "\n",
                "for doc in train_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in dev_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in test_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "print(f\"Nettoyage terminé pour {len(train_data)} docs train, {len(dev_data)} docs dev, {len(test_data)} docs test.\")\n",
                "print(f\"\\nExemple de texte nettoyé (doc 0) : {train_data[0]['cleaned_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 4 : Tokenization, POS Tagging et Lemmatization\n",
                "Objectif : Tokeniser, identifier les parties du discours (POS) et lemmatiser les textes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonctions tokenize_and_pos() et lemmatize_text() définies.\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_and_pos(text):\n",
                "    \"\"\"\n",
                "    Tokenise le texte et effectue le POS tagging avec NLTK.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à tokeniser\n",
                "    \n",
                "    Returns:\n",
                "        list: Liste de tuples (token, pos_tag)\n",
                "    \"\"\"\n",
                "    tokens = word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    return pos_tags\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à lemmatiser\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte lemmatisé\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "print(\"Fonctions tokenize_and_pos() et lemmatize_text() définies.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte nettoyé (extrait) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate understanding of the surrounding scene is crucial to navigation and action planning current state of the art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole while these networks exhibit outstanding recognition perf\n",
                        "\n",
                        "--- Tokenization + POS Tagging ---\n",
                        "Nombre de tokens : 70\n",
                        "Premiers 10 tokens avec POS : [('full', 'JJ'), ('resolution', 'NN'), ('residual', 'JJ'), ('networks', 'NNS'), ('for', 'IN'), ('semantic', 'JJ'), ('segmentation', 'NN'), ('in', 'IN'), ('street', 'NN'), ('scenes', 'NNS')]\n",
                        "\n",
                        "--- Lemmatization ---\n",
                        "Texte lemmatisé (extrait) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of modern autonomous driving system as an accurate und...\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'cleaned_text' in train_data[0]:\n",
                "    sample_text = train_data[0]['cleaned_text'][:500]  # Premier 500 caractères\n",
                "    \n",
                "    print(\"Texte nettoyé (extrait) :\")\n",
                "    print(sample_text)\n",
                "    \n",
                "    print(\"\\n--- Tokenization + POS Tagging ---\")\n",
                "    pos_tags = tokenize_and_pos(sample_text)\n",
                "    print(f\"Nombre de tokens : {len(pos_tags)}\")\n",
                "    print(f\"Premiers 10 tokens avec POS : {pos_tags[:10]}\")\n",
                "    \n",
                "    print(\"\\n--- Lemmatization ---\")\n",
                "    lemmatized = lemmatize_text(sample_text)\n",
                "    print(f\"Texte lemmatisé (extrait) : {lemmatized[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application de la lemmatization sur les 100 premiers documents train...\n",
                        "  Traité 20/100 documents...\n",
                        "  Traité 40/100 documents...\n",
                        "  Traité 60/100 documents...\n",
                        "  Traité 80/100 documents...\n",
                        "  Traité 100/100 documents...\n",
                        "Lemmatization terminée pour le sous-ensemble.\n",
                        "\n",
                        "Exemple de texte lemmatisé (doc 0) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of m...\n"
                    ]
                }
            ],
            "source": [
                "# Application sur tous les documents (limité aux 100 premiers pour la démo)\n",
                "# Pour le dataset complet, cela peut prendre du temps\n",
                "print(\"Application de la lemmatization sur les 100 premiers documents train...\")\n",
                "\n",
                "for i, doc in enumerate(train_data[:100]):\n",
                "    if 'cleaned_text' in doc:\n",
                "        doc['lemmatized_text'] = lemmatize_text(doc['cleaned_text'])\n",
                "    if (i + 1) % 20 == 0:\n",
                "        print(f\"  Traité {i + 1}/100 documents...\")\n",
                "\n",
                "print(\"Lemmatization terminée pour le sous-ensemble.\")\n",
                "print(f\"\\nExemple de texte lemmatisé (doc 0) : {train_data[0]['lemmatized_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 5 : Représentation Vectorielle TF-IDF\n",
                "Objectif : Créer une représentation vectorielle des textes avec TF-IDF."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calcul de la matrice TF-IDF...\n",
                        "\n",
                        "Matrice TF-IDF créée :\n",
                        "  Forme : (306, 5000)\n",
                        "  Nombre de documents : 306\n",
                        "  Nombre de features : 5000\n",
                        "  Densité : 0.2370\n"
                    ]
                }
            ],
            "source": [
                "# Préparation des textes pour TF-IDF (utilisation des textes nettoyés)\n",
                "train_texts = [doc['cleaned_text'] for doc in train_data if 'cleaned_text' in doc]\n",
                "\n",
                "# Création du vectoriseur TF-IDF\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_features=5000,  # Limite à 5000 features les plus importantes\n",
                "    min_df=2,           # Ignore les termes qui apparaissent dans moins de 2 documents\n",
                "    max_df=0.8,         # Ignore les termes qui apparaissent dans plus de 80% des documents\n",
                "    ngram_range=(1, 2)  # Unigrammes et bigrammes\n",
                ")\n",
                "\n",
                "# Calcul de la matrice TF-IDF\n",
                "print(\"Calcul de la matrice TF-IDF...\")\n",
                "tfidf_matrix = tfidf_vectorizer.fit_transform(train_texts)\n",
                "\n",
                "print(f\"\\nMatrice TF-IDF créée :\")\n",
                "print(f\"  Forme : {tfidf_matrix.shape}\")\n",
                "print(f\"  Nombre de documents : {tfidf_matrix.shape[0]}\")\n",
                "print(f\"  Nombre de features : {tfidf_matrix.shape[1]}\")\n",
                "print(f\"  Densité : {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 10 features TF-IDF pour le document 0 :\n",
                        "  stream: 0.2819\n",
                        "  cityscapes: 0.2445\n",
                        "  residual: 0.2226\n",
                        "  resolution: 0.1905\n",
                        "  segmentation: 0.1893\n",
                        "  reference reference: 0.1770\n",
                        "  pooling: 0.1688\n",
                        "  pooling operations: 0.1449\n",
                        "  image: 0.1281\n",
                        "  semantic segmentation: 0.1195\n"
                    ]
                }
            ],
            "source": [
                "# Affichage des top features pour le premier document\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "doc_0_vector = tfidf_matrix[0].toarray()[0]\n",
                "top_indices = doc_0_vector.argsort()[-10:][::-1]\n",
                "\n",
                "print(\"Top 10 features TF-IDF pour le document 0 :\")\n",
                "for idx in top_indices:\n",
                "    print(f\"  {feature_names[idx]}: {doc_0_vector[idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Résumé des Étapes Complétées\n",
                "\n",
                "✅ **Étape 1** : Setup et importations  \n",
                "✅ **Étape 2** : Chargement et exploration SciREX (306 train, 66 dev, 66 test)  \n",
                "✅ **Étape 3** : Nettoyage et normalisation  \n",
                "✅ **Étape 4** : Tokenization, POS tagging et lemmatization  \n",
                "✅ **Étape 5** : Représentation TF-IDF (matrice 306×5000)\n",
                "\n",
                "**Note** : Les embeddings BERT ont été retirés pour des raisons de performance. TF-IDF est suffisant pour la démonstration pédagogique."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
