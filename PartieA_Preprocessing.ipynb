{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Représentation Text\n",
                "\n",
                "**Université Paris Cité - Master 2 VMI**\n",
                "**Cours :** IFLCE085 Recherche et extraction sémantique à partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**Équipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## Étape 1 : Setup et Importations\n",
                "Objectif : Configurer l'environnement et importer les librairies nécessaires.\n",
                "\n",
                "**Note importante** : Sur Mac ARM avec Python 3.8, spaCy doit être installé via conda (pas pip)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
                        "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
                        "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
                        "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
                        "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
                        "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
                        "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
                        "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
                        "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
                        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
                        "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
                        "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
                        "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
                        "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
                        "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
                        "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
                        "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
                        "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
                        "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
                        "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
                        "Collecting en-core-web-sm==3.8.0\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
                        "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n",
                        "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
                        "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
                        "order to load all the package's dependencies. You can do this by selecting the\n",
                        "'Restart kernel' or 'Restart runtime' option.\n",
                        "Environnement configuré avec succès.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "# Installation des dépendances de base\n",
                "!{sys.executable} -m pip install pandas\n",
                "!{sys.executable} -m pip install numpy\n",
                "!{sys.executable} -m pip install nltk\n",
                "!{sys.executable} -m pip install scikit-learn\n",
                "!{sys.executable} -m pip install transformers\n",
                "!{sys.executable} -m pip install torch\n",
                "!{sys.executable} -m pip install spacy\n",
                "\n",
                "# Téléchargement du modèle spaCy anglais\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# Téléchargement des ressources NLTK (version mise à jour)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du modèle spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"Environnement configuré avec succès.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 2 : Chargement et Exploration des Données (SciREX)\n",
                "Objectif : Télécharger (si nécessaire) et charger le dataset SciREX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import urllib.request\n",
                "import tarfile\n",
                "\n",
                "DATA_DIR = \"release_data\"\n",
                "DATA_URL = \"https://github.com/allenai/SciREX/raw/master/scirex_dataset/release_data.tar.gz\"\n",
                "TAR_FILE = \"release_data.tar.gz\"\n",
                "\n",
                "def download_and_extract_data():\n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(\"Dossier de données non trouvé. Téléchargement en cours...\")\n",
                "        try:\n",
                "            urllib.request.urlretrieve(DATA_URL, TAR_FILE)\n",
                "            print(\"Téléchargement terminé. Extraction...\")\n",
                "            with tarfile.open(TAR_FILE, \"r:gz\") as tar:\n",
                "                tar.extractall()\n",
                "            print(\"Extraction terminée.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Erreur lors du téléchargement/extraction : {e}\")\n",
                "    else:\n",
                "        print(\"Les données sont déjà présentes.\")\n",
                "\n",
                "download_and_extract_data()\n",
                "\n",
                "FILES = {\n",
                "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
                "    \"dev\": os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
                "    \"test\": os.path.join(DATA_DIR, \"test.jsonl\")\n",
                "}\n",
                "\n",
                "def load_jsonl(file_path):\n",
                "    \"\"\"Charge un fichier JSONL dans une liste de dictionnaires.\"\"\"\n",
                "    data = []\n",
                "    if os.path.exists(file_path):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                data.append(json.loads(line))\n",
                "    else:\n",
                "        print(f\"Attention : Fichier {file_path} introuvable.\")\n",
                "    return data\n",
                "\n",
                "# Chargement des données\n",
                "print(\"Chargement des données...\")\n",
                "train_data = load_jsonl(FILES[\"train\"])\n",
                "dev_data = load_jsonl(FILES[\"dev\"])\n",
                "test_data = load_jsonl(FILES[\"test\"])\n",
                "\n",
                "print(f\"Nombre de documents Train : {len(train_data)}\")\n",
                "print(f\"Nombre de documents Dev   : {len(dev_data)}\")\n",
                "print(f\"Nombre de documents Test  : {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exploration d'un document type\n",
                "if train_data:\n",
                "    doc_example = train_data[0]\n",
                "    print(\"\\nClés disponibles dans un document :\")\n",
                "    print(list(doc_example.keys()))\n",
                "\n",
                "    print(\"\\nExemple de contenu (champs principaux) :\")\n",
                "    print(f\"ID: {doc_example.get('doc_id')}\")\n",
                "    if 'words' in doc_example:\n",
                "        print(f\"Début du texte (50 premiers mots) : {' '.join(doc_example['words'][:50])}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 3 : Nettoyage et Normalisation\n",
                "Objectif : Nettoyer le texte (lowercase, suppression caractères spéciaux, espaces multiples)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caractères spéciaux, espaces multiples.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à nettoyer\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte nettoyé\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caractères spéciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "print(\"Fonction clean_text() définie.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'words' in doc_example:\n",
                "    raw_text = ' '.join(doc_example['words'])\n",
                "    cleaned_text = clean_text(raw_text)\n",
                "    \n",
                "    print(\"Texte original (200 premiers caractères) :\")\n",
                "    print(raw_text[:200])\n",
                "    print(\"\\nTexte nettoyé (200 premiers caractères) :\")\n",
                "    print(cleaned_text[:200])\n",
                "    print(f\"\\nLongueur originale : {len(raw_text)} caractères\")\n",
                "    print(f\"Longueur nettoyée  : {len(cleaned_text)} caractères\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Application du nettoyage sur tous les documents\n",
                "print(\"Application du nettoyage sur tous les documents...\")\n",
                "\n",
                "for doc in train_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in dev_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in test_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "print(f\"Nettoyage terminé pour {len(train_data)} docs train, {len(dev_data)} docs dev, {len(test_data)} docs test.\")\n",
                "print(f\"\\nExemple de texte nettoyé (doc 0) : {train_data[0]['cleaned_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 4 : Tokenization, POS Tagging et Lemmatization\n",
                "Objectif : Tokeniser, identifier les parties du discours (POS) et lemmatiser les textes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonctions tokenize_and_pos() et lemmatize_text() définies.\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_and_pos(text):\n",
                "    \"\"\"\n",
                "    Tokenise le texte et effectue le POS tagging avec NLTK.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à tokeniser\n",
                "    \n",
                "    Returns:\n",
                "        list: Liste de tuples (token, pos_tag)\n",
                "    \"\"\"\n",
                "    tokens = word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    return pos_tags\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à lemmatiser\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte lemmatisé\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "print(\"Fonctions tokenize_and_pos() et lemmatize_text() définies.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte nettoyé (extrait) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate understanding of the surrounding scene is crucial to navigation and action planning current state of the art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole while these networks exhibit outstanding recognition perf\n",
                        "\n",
                        "--- Tokenization + POS Tagging ---\n",
                        "Nombre de tokens : 70\n",
                        "Premiers 10 tokens avec POS : [('full', 'JJ'), ('resolution', 'NN'), ('residual', 'JJ'), ('networks', 'NNS'), ('for', 'IN'), ('semantic', 'JJ'), ('segmentation', 'NN'), ('in', 'IN'), ('street', 'NN'), ('scenes', 'NNS')]\n",
                        "\n",
                        "--- Lemmatization ---\n",
                        "Texte lemmatisé (extrait) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of modern autonomous driving system as an accurate und...\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'cleaned_text' in train_data[0]:\n",
                "    sample_text = train_data[0]['cleaned_text'][:500]  # Premier 500 caractères\n",
                "    \n",
                "    print(\"Texte nettoyé (extrait) :\")\n",
                "    print(sample_text)\n",
                "    \n",
                "    print(\"\\n--- Tokenization + POS Tagging ---\")\n",
                "    pos_tags = tokenize_and_pos(sample_text)\n",
                "    print(f\"Nombre de tokens : {len(pos_tags)}\")\n",
                "    print(f\"Premiers 10 tokens avec POS : {pos_tags[:10]}\")\n",
                "    \n",
                "    print(\"\\n--- Lemmatization ---\")\n",
                "    lemmatized = lemmatize_text(sample_text)\n",
                "    print(f\"Texte lemmatisé (extrait) : {lemmatized[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application de la lemmatization sur les 100 premiers documents train...\n",
                        "  Traité 20/100 documents...\n",
                        "  Traité 40/100 documents...\n",
                        "  Traité 60/100 documents...\n",
                        "  Traité 80/100 documents...\n",
                        "  Traité 100/100 documents...\n",
                        "Lemmatization terminée pour le sous-ensemble.\n",
                        "\n",
                        "Exemple de texte lemmatisé (doc 0) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of m...\n"
                    ]
                }
            ],
            "source": [
                "# Application sur tous les documents (limité aux 100 premiers pour la démo)\n",
                "# Pour le dataset complet, cela peut prendre du temps\n",
                "print(\"Application de la lemmatization sur les 100 premiers documents train...\")\n",
                "\n",
                "for i, doc in enumerate(train_data[:100]):\n",
                "    if 'cleaned_text' in doc:\n",
                "        doc['lemmatized_text'] = lemmatize_text(doc['cleaned_text'])\n",
                "    if (i + 1) % 20 == 0:\n",
                "        print(f\"  Traité {i + 1}/100 documents...\")\n",
                "\n",
                "print(\"Lemmatization terminée pour le sous-ensemble.\")\n",
                "print(f\"\\nExemple de texte lemmatisé (doc 0) : {train_data[0]['lemmatized_text'][:150]}...\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
