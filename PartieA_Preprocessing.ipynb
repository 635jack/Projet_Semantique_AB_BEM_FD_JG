{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Repr√©sentation Text\n",
                "\n",
                "**Universit√© Paris Cit√© - Master 2 VMI**  \n",
                "**Cours :** IFLCE085 Recherche et extraction s√©mantique √† partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**√âquipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## Dataset : NER (Named Entity Recognition)\n",
                "\n",
                "Ce notebook traite un dataset de **2221 phrases** annot√©es pour la reconnaissance d'entit√©s nomm√©es.\n",
                "\n",
                "**Colonnes :**\n",
                "- `id` : Identifiant unique de la phrase\n",
                "- `words` : Liste des mots tokenis√©s\n",
                "- `ner_tags` : Tags NER (0=O, 1=B-LOC, 2=B-PER, 4=B-ORG)\n",
                "- `text` : Texte brut de la phrase"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 1 : Setup et Importations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting en-core-web-sm==3.7.1\n",
                        "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='github.com', port=443): Read timed out. (read timeout=15)\")': /explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\u001b[0m\u001b[33m\n",
                        "\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
                        "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
                        "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
                        "Requirement already satisfied: jinja2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
                        "Requirement already satisfied: setuptools in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.3.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
                        "Requirement already satisfied: numpy>=1.15.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.27.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.20)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
                        "Requirement already satisfied: click>=8.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
                        "Requirement already satisfied: rich>=10.11.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
                        "Requirement already satisfied: wrapt in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
                        "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n",
                        "‚úÖ Environnement configur√© avec succ√®s.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "# Installation des d√©pendances de base\n",
                "!{sys.executable} -m pip install -q pandas numpy nltk scikit-learn spacy\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pickle\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from scipy.sparse import save_npz\n",
                "\n",
                "# T√©l√©chargement des ressources NLTK\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du mod√®le spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"‚úÖ Environnement configur√© avec succ√®s.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 2 : Chargement et Exploration des Donn√©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Dataset charg√© : 700 phrases\n",
                        "\n",
                        "Colonnes : ['id', 'words', 'ner_tags', 'text']\n",
                        "\n",
                        "Aper√ßu des donn√©es :\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>words</th>\n",
                            "      <th>ner_tags</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>en-doc5809-sent11</td>\n",
                            "      <td>['When' 'Aeneas' 'later' 'traveled' 'to' 'Hades' ',' 'he' 'called' 'to'\\n 'her' 'ghost' 'but' 's...</td>\n",
                            "      <td>[0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
                            "      <td>When Aeneas later traveled to Hades , he called to her ghost but she neither spoke to nor acknow...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>en-doc6123-sent45</td>\n",
                            "      <td>['On' '23' 'November' '1969' 'he' 'wrote' 'to' 'The' 'Times' 'newspaper'\\n 'saying' 'that' 'the'...</td>\n",
                            "      <td>[0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]</td>\n",
                            "      <td>On 23 November 1969 he wrote to The Times newspaper saying that the preparation for show trials ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>en-doc5831-sent40</td>\n",
                            "      <td>['Stephenson' \"'s\" 'estimates' 'and' 'organising' 'ability' 'proved' 'to'\\n 'be' 'inferior' 'to'...</td>\n",
                            "      <td>[2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0]</td>\n",
                            "      <td>Stephenson 's estimates and organising ability proved to be inferior to those of Locke and the b...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>en-doc6189-sent73</td>\n",
                            "      <td>['France' 'then' 'postponed' 'a' 'visit' 'by' 'Sharon' '.']</td>\n",
                            "      <td>[1 0 0 0 0 0 2 0]</td>\n",
                            "      <td>France then postponed a visit by Sharon .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>en-doc6139-sent18</td>\n",
                            "      <td>['Only' 'twenty-seven' 'years' 'old' 'at' 'his' 'death' ',' 'Moseley'\\n 'could' 'in' 'many' 'sci...</td>\n",
                            "      <td>[0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
                            "      <td>Only twenty-seven years old at his death , Moseley could in many scientists ' opinions have cont...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                  id  \\\n",
                            "0  en-doc5809-sent11   \n",
                            "1  en-doc6123-sent45   \n",
                            "2  en-doc5831-sent40   \n",
                            "3  en-doc6189-sent73   \n",
                            "4  en-doc6139-sent18   \n",
                            "\n",
                            "                                                                                                 words  \\\n",
                            "0  ['When' 'Aeneas' 'later' 'traveled' 'to' 'Hades' ',' 'he' 'called' 'to'\\n 'her' 'ghost' 'but' 's...   \n",
                            "1  ['On' '23' 'November' '1969' 'he' 'wrote' 'to' 'The' 'Times' 'newspaper'\\n 'saying' 'that' 'the'...   \n",
                            "2  ['Stephenson' \"'s\" 'estimates' 'and' 'organising' 'ability' 'proved' 'to'\\n 'be' 'inferior' 'to'...   \n",
                            "3                                          ['France' 'then' 'postponed' 'a' 'visit' 'by' 'Sharon' '.']   \n",
                            "4  ['Only' 'twenty-seven' 'years' 'old' 'at' 'his' 'death' ',' 'Moseley'\\n 'could' 'in' 'many' 'sci...   \n",
                            "\n",
                            "                                                      ner_tags  \\\n",
                            "0                  [0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
                            "1          [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]   \n",
                            "2          [2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0]   \n",
                            "3                                            [1 0 0 0 0 0 2 0]   \n",
                            "4  [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
                            "\n",
                            "                                                                                                  text  \n",
                            "0  When Aeneas later traveled to Hades , he called to her ghost but she neither spoke to nor acknow...  \n",
                            "1  On 23 November 1969 he wrote to The Times newspaper saying that the preparation for show trials ...  \n",
                            "2  Stephenson 's estimates and organising ability proved to be inferior to those of Locke and the b...  \n",
                            "3                                                            France then postponed a visit by Sharon .  \n",
                            "4  Only twenty-seven years old at his death , Moseley could in many scientists ' opinions have cont...  "
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Chargement du dataset\n",
                "df = pd.read_csv('data.csv')\n",
                "\n",
                "print(f\"üìä Dataset charg√© : {len(df)} phrases\")\n",
                "print(f\"\\nColonnes : {list(df.columns)}\")\n",
                "print(f\"\\nAper√ßu des donn√©es :\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìà Statistiques du dataset :\n",
                        "  Nombre total de phrases : 700\n",
                        "  Longueur moyenne du texte : 132.6 caract√®res\n",
                        "  Longueur min : 11 caract√®res\n",
                        "  Longueur max : 509 caract√®res\n",
                        "\n",
                        "Exemple de phrase :\n",
                        "  ID: en-doc5809-sent11\n",
                        "  Texte: When Aeneas later traveled to Hades , he called to her ghost but she neither spoke to nor acknowledged him .\n"
                    ]
                }
            ],
            "source": [
                "# Statistiques\n",
                "print(\"üìà Statistiques du dataset :\")\n",
                "print(f\"  Nombre total de phrases : {len(df)}\")\n",
                "print(f\"  Longueur moyenne du texte : {df['text'].str.len().mean():.1f} caract√®res\")\n",
                "print(f\"  Longueur min : {df['text'].str.len().min()} caract√®res\")\n",
                "print(f\"  Longueur max : {df['text'].str.len().max()} caract√®res\")\n",
                "print(f\"\\nExemple de phrase :\")\n",
                "print(f\"  ID: {df.iloc[0]['id']}\")\n",
                "print(f\"  Texte: {df.iloc[0]['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 3 : Split Train/Dev/Test\n",
                "\n",
                "Division du dataset en 3 ensembles :\n",
                "- **Train** : 70% (1554 phrases)\n",
                "- **Dev** : 15% (333 phrases)\n",
                "- **Test** : 15% (334 phrases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÇ Split effectu√© :\n",
                        "  Train : 490 phrases (70.0%)\n",
                        "  Dev   : 105 phrases (15.0%)\n",
                        "  Test  : 105 phrases (15.0%)\n",
                        "  Total : 700 phrases\n"
                    ]
                }
            ],
            "source": [
                "# Split strat√©gique : 70% train, 15% dev, 15% test\n",
                "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
                "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
                "\n",
                "print(\"üìÇ Split effectu√© :\")\n",
                "print(f\"  Train : {len(train_df)} phrases ({len(train_df)/len(df)*100:.1f}%)\")\n",
                "print(f\"  Dev   : {len(dev_df)} phrases ({len(dev_df)/len(df)*100:.1f}%)\")\n",
                "print(f\"  Test  : {len(test_df)} phrases ({len(test_df)/len(df)*100:.1f}%)\")\n",
                "print(f\"  Total : {len(train_df) + len(dev_df) + len(test_df)} phrases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 4 : Nettoyage et Normalisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Nettoyage termin√©\n",
                        "\n",
                        "Exemple :\n",
                        "  Original : According to the conservative think tank Heritage Foundation , Hungary 's economy was 67.2 percent \" free \" in 2008 , which makes it the world 's 43rd-freest economy .\n",
                        "  Nettoy√©  : according to the conservative think tank heritage foundation hungary s economy was 67 2 percent free in 2008 which makes it the world s 43rd freest economy\n"
                    ]
                }
            ],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caract√®res sp√©ciaux, normalisation espaces.\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caract√®res sp√©ciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "# Application du nettoyage\n",
                "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
                "dev_df['cleaned_text'] = dev_df['text'].apply(clean_text)\n",
                "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
                "\n",
                "print(\"‚úÖ Nettoyage termin√©\")\n",
                "print(f\"\\nExemple :\")\n",
                "print(f\"  Original : {train_df.iloc[0]['text']}\")\n",
                "print(f\"  Nettoy√©  : {train_df.iloc[0]['cleaned_text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 5 : Lemmatization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Lemmatization de 490 phrases (TRAIN)...\n",
                        "‚úÖ Lemmatization termin√©e pour TRAIN\n",
                        "üîÑ Lemmatization de 105 phrases (DEV)...\n",
                        "‚úÖ Lemmatization termin√©e pour DEV\n",
                        "üîÑ Lemmatization de 105 phrases (TEST)...\n",
                        "‚úÖ Lemmatization termin√©e pour TEST\n",
                        "\n",
                        "Exemple de texte lemmatis√© :\n",
                        "  accord to the conservative think tank heritage foundation hungary s economy be 67 2 percent free in ...\n"
                    ]
                }
            ],
            "source": [
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "def process_dataset_lemmatization(df, name):\n",
                "    \"\"\"Applique la lemmatization sur un dataset.\"\"\"\n",
                "    print(f\"üîÑ Lemmatization de {len(df)} phrases ({name})...\")\n",
                "    df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
                "    print(f\"‚úÖ Lemmatization termin√©e pour {name}\")\n",
                "\n",
                "# Application sur les 3 ensembles\n",
                "process_dataset_lemmatization(train_df, \"TRAIN\")\n",
                "process_dataset_lemmatization(dev_df, \"DEV\")\n",
                "process_dataset_lemmatization(test_df, \"TEST\")\n",
                "\n",
                "print(f\"\\nExemple de texte lemmatis√© :\")\n",
                "print(f\"  {train_df.iloc[0]['lemmatized_text'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 6 : Repr√©sentation Vectorielle TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Calcul TF-IDF...\n",
                        "\n",
                        "‚úÖ Matrices TF-IDF cr√©√©es :\n",
                        "  TRAIN : (490, 1680) (Densit√© : 0.0103)\n",
                        "  DEV   : (105, 1680) (Densit√© : 0.0088)\n",
                        "  TEST  : (105, 1680) (Densit√© : 0.0089)\n"
                    ]
                }
            ],
            "source": [
                "# Pr√©paration des textes lemmatis√©s\n",
                "train_texts = train_df['lemmatized_text'].tolist()\n",
                "dev_texts = dev_df['lemmatized_text'].tolist()\n",
                "test_texts = test_df['lemmatized_text'].tolist()\n",
                "\n",
                "# Cr√©ation du vectoriseur TF-IDF\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_features=3000,  # Limite √† 3000 features (dataset plus petit)\n",
                "    min_df=2,\n",
                "    max_df=0.8,\n",
                "    ngram_range=(1, 2)\n",
                ")\n",
                "\n",
                "# Fit sur TRAIN, transform sur tous\n",
                "print(\"üîÑ Calcul TF-IDF...\")\n",
                "tfidf_train = tfidf_vectorizer.fit_transform(train_texts)\n",
                "tfidf_dev = tfidf_vectorizer.transform(dev_texts)\n",
                "tfidf_test = tfidf_vectorizer.transform(test_texts)\n",
                "\n",
                "print(f\"\\n‚úÖ Matrices TF-IDF cr√©√©es :\")\n",
                "print(f\"  TRAIN : {tfidf_train.shape} (Densit√© : {tfidf_train.nnz / (tfidf_train.shape[0] * tfidf_train.shape[1]):.4f})\")\n",
                "print(f\"  DEV   : {tfidf_dev.shape} (Densit√© : {tfidf_dev.nnz / (tfidf_dev.shape[0] * tfidf_dev.shape[1]):.4f})\")\n",
                "print(f\"  TEST  : {tfidf_test.shape} (Densit√© : {tfidf_test.nnz / (tfidf_test.shape[0] * tfidf_test.shape[1]):.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîù Top 10 features TF-IDF (phrase 0) :\n",
                        "  economy: 0.4162\n",
                        "  free: 0.3812\n",
                        "  foundation: 0.2271\n",
                        "  heritage: 0.2271\n",
                        "  economy be: 0.2271\n",
                        "  make it: 0.2271\n",
                        "  it the: 0.2271\n",
                        "  in 2008: 0.2271\n",
                        "  think: 0.2164\n",
                        "  hungary: 0.2081\n"
                    ]
                }
            ],
            "source": [
                "# Top features pour la premi√®re phrase\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "doc_0_vector = tfidf_train[0].toarray()[0]\n",
                "top_indices = doc_0_vector.argsort()[-10:][::-1]\n",
                "\n",
                "print(\"üîù Top 10 features TF-IDF (phrase 0) :\")\n",
                "for idx in top_indices:\n",
                "    if doc_0_vector[idx] > 0:\n",
                "        print(f\"  {feature_names[idx]}: {doc_0_vector[idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 7 : Export des R√©sultats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ Dossier de sortie : preprocessed_data/\n"
                    ]
                }
            ],
            "source": [
                "# Cr√©ation du dossier de sortie\n",
                "OUTPUT_DIR = \"preprocessed_data\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"üìÅ Dossier de sortie : {OUTPUT_DIR}/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ CSV export√©s\n",
                        "  train_preprocessed.csv : 490 lignes\n",
                        "  dev_preprocessed.csv   : 105 lignes\n",
                        "  test_preprocessed.csv  : 105 lignes\n"
                    ]
                }
            ],
            "source": [
                "# Export des CSV\n",
                "train_df[['id', 'text', 'cleaned_text', 'lemmatized_text']].to_csv(\n",
                "    os.path.join(OUTPUT_DIR, 'train_preprocessed.csv'), index=False\n",
                ")\n",
                "dev_df[['id', 'text', 'cleaned_text', 'lemmatized_text']].to_csv(\n",
                "    os.path.join(OUTPUT_DIR, 'dev_preprocessed.csv'), index=False\n",
                ")\n",
                "test_df[['id', 'text', 'cleaned_text', 'lemmatized_text']].to_csv(\n",
                "    os.path.join(OUTPUT_DIR, 'test_preprocessed.csv'), index=False\n",
                ")\n",
                "\n",
                "print(\"‚úÖ CSV export√©s\")\n",
                "print(f\"  train_preprocessed.csv : {len(train_df)} lignes\")\n",
                "print(f\"  dev_preprocessed.csv   : {len(dev_df)} lignes\")\n",
                "print(f\"  test_preprocessed.csv  : {len(test_df)} lignes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Matrices TF-IDF et vectoriseur export√©s\n"
                    ]
                }
            ],
            "source": [
                "# Export des matrices TF-IDF\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix.npz'), tfidf_train)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_dev.npz'), tfidf_dev)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_test.npz'), tfidf_test)\n",
                "\n",
                "# Export du vectoriseur\n",
                "with open(os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
                "    pickle.dump(tfidf_vectorizer, f)\n",
                "\n",
                "# Export des feature names\n",
                "np.save(os.path.join(OUTPUT_DIR, 'tfidf_feature_names.npy'), feature_names)\n",
                "\n",
                "print(\"‚úÖ Matrices TF-IDF et vectoriseur export√©s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ M√©tadonn√©es export√©es\n"
                    ]
                }
            ],
            "source": [
                "# M√©tadonn√©es\n",
                "metadata = {\n",
                "    'dataset': 'NER Dataset',\n",
                "    'total_sentences': len(df),\n",
                "    'train_size': len(train_df),\n",
                "    'dev_size': len(dev_df),\n",
                "    'test_size': len(test_df),\n",
                "    'tfidf_features': len(feature_names),\n",
                "    'preprocessing_steps': [\n",
                "        '1. Lowercase',\n",
                "        '2. Suppression caract√®res sp√©ciaux',\n",
                "        '3. Normalisation espaces',\n",
                "        '4. Lemmatization (spaCy)',\n",
                "        '5. TF-IDF (max_features=3000, ngram_range=(1,2))'\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(\"‚úÖ M√©tadonn√©es export√©es\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üì¶ R√âSUM√â DES FICHIERS EXPORT√âS\n",
                        "============================================================\n",
                        "  dev_preprocessed.csv                (0.04 MB)\n",
                        "  metadata.json                       (0.00 MB)\n",
                        "  test_preprocessed.csv               (0.04 MB)\n",
                        "  tfidf_feature_names.npy             (0.03 MB)\n",
                        "  tfidf_matrix.npz                    (0.06 MB)\n",
                        "  tfidf_matrix_dev.npz                (0.01 MB)\n",
                        "  tfidf_matrix_test.npz               (0.01 MB)\n",
                        "  tfidf_vectorizer.pkl                (0.20 MB)\n",
                        "  train_preprocessed.csv              (0.18 MB)\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# R√©sum√© des fichiers\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üì¶ R√âSUM√â DES FICHIERS EXPORT√âS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    if os.path.isfile(filepath):\n",
                "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
                "        print(f\"  {filename:35s} ({size_mb:.2f} MB)\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## R√©sum√© Technique\n",
                "\n",
                "### Dataset\n",
                "- **Source** : NER Dataset (Named Entity Recognition)\n",
                "- **Total** : 2221 phrases\n",
                "- **Split** : Train (70%), Dev (15%), Test (15%)\n",
                "\n",
                "### Pipeline de Preprocessing\n",
                "1. **Nettoyage** : Lowercase, suppression caract√®res sp√©ciaux, normalisation espaces\n",
                "2. **Lemmatization** : spaCy `en_core_web_sm`\n",
                "3. **TF-IDF** : 3000 features, bigrammes (1,2)\n",
                "\n",
                "### Fichiers Export√©s\n",
                "- `train_preprocessed.csv`, `dev_preprocessed.csv`, `test_preprocessed.csv`\n",
                "- `tfidf_matrix.npz`, `tfidf_matrix_dev.npz`, `tfidf_matrix_test.npz`\n",
                "- `tfidf_vectorizer.pkl`, `tfidf_feature_names.npy`\n",
                "- `metadata.json`\n",
                "\n",
                "### Utilisation (Partie B)\n",
                "```python\n",
                "import pandas as pd\n",
                "from scipy.sparse import load_npz\n",
                "import pickle\n",
                "\n",
                "# Charger les donn√©es\n",
                "df_train = pd.read_csv('preprocessed_data/train_preprocessed.csv')\n",
                "tfidf_train = load_npz('preprocessed_data/tfidf_matrix.npz')\n",
                "\n",
                "# Charger le vectoriseur\n",
                "with open('preprocessed_data/tfidf_vectorizer.pkl', 'rb') as f:\n",
                "    vectorizer = pickle.load(f)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tf-metal",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
