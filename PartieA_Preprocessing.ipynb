{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Repr√©sentation Text\n",
                "\n",
                "**Universit√© Paris Cit√© - Master 2 VMI**  \n",
                "**Cours :** IFLCE085 Recherche et extraction s√©mantique √† partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**√âquipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## Dataset : NER (Named Entity Recognition)\n",
                "\n",
                "Ce notebook traite un dataset de **2221 phrases** annot√©es pour la reconnaissance d'entit√©s nomm√©es.\n",
                "\n",
                "**Colonnes originales :**\n",
                "- `id` : Identifiant unique de la phrase\n",
                "- `words` : Liste des mots tokenis√©s\n",
                "- `ner_tags` : Tags NER (0=O, 1=B-LOC, 2=B-PER, 4=B-ORG)\n",
                "- `text` : Texte brut de la phrase\n",
                "\n",
                "**Colonnes ajout√©es par ce notebook :**\n",
                "- `cleaned_text` : Texte nettoy√© (lowercase, sans caract√®res sp√©ciaux)\n",
                "- `lemmatized_text` : Texte lemmatis√© avec spaCy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 1 : Setup et Importations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "# Installation des d√©pendances de base\n",
                "!{sys.executable} -m pip install -q pandas numpy nltk scikit-learn spacy\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "import os\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "\n",
                "# Chargement du mod√®le spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"‚úÖ Environnement configur√© avec succ√®s.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 2 : Chargement des Donn√©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement du dataset\n",
                "df = pd.read_csv('data.csv')\n",
                "\n",
                "print(f\"üìä Dataset charg√© : {len(df)} phrases\")\n",
                "print(f\"\\nColonnes originales : {list(df.columns)}\")\n",
                "print(f\"\\nAper√ßu des donn√©es :\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 3 : Nettoyage du Texte\n",
                "\n",
                "Ajout de la colonne `cleaned_text` au dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caract√®res sp√©ciaux, normalisation espaces.\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caract√®res sp√©ciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "# Application du nettoyage\n",
                "print(\"üîÑ Nettoyage en cours...\")\n",
                "df['cleaned_text'] = df['text'].apply(clean_text)\n",
                "\n",
                "print(\"‚úÖ Colonne 'cleaned_text' ajout√©e\")\n",
                "print(f\"\\nExemple :\")\n",
                "print(f\"  Original : {df.iloc[0]['text']}\")\n",
                "print(f\"  Nettoy√©  : {df.iloc[0]['cleaned_text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 4 : Lemmatization\n",
                "\n",
                "Ajout de la colonne `lemmatized_text` au dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "# Application de la lemmatization\n",
                "print(\"üîÑ Lemmatization en cours (cela peut prendre quelques minutes)...\")\n",
                "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
                "\n",
                "print(\"‚úÖ Colonne 'lemmatized_text' ajout√©e\")\n",
                "print(f\"\\nExemple de texte lemmatis√© :\")\n",
                "print(f\"  {df.iloc[0]['lemmatized_text'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 5 : Sauvegarde du Dataset Enrichi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# V√©rification des colonnes\n",
                "print(\"üìã Colonnes du dataset enrichi :\")\n",
                "print(list(df.columns))\n",
                "print(f\"\\nNombre de lignes : {len(df)}\")\n",
                "\n",
                "# Aper√ßu\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sauvegarde du dataset enrichi\n",
                "output_file = 'data_preprocessed.csv'\n",
                "df.to_csv(output_file, index=False)\n",
                "\n",
                "print(f\"‚úÖ Dataset enrichi sauvegard√© : {output_file}\")\n",
                "print(f\"   Colonnes : {list(df.columns)}\")\n",
                "print(f\"   Nombre de lignes : {len(df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## R√©sum√©\n",
                "\n",
                "### Dataset Enrichi\n",
                "\n",
                "Le fichier `data_preprocessed.csv` contient maintenant **6 colonnes** :\n",
                "\n",
                "**Colonnes originales (conserv√©es) :**\n",
                "1. `id` : Identifiant unique\n",
                "2. `words` : Liste des mots tokenis√©s\n",
                "3. `ner_tags` : Tags NER\n",
                "4. `text` : Texte original\n",
                "\n",
                "**Colonnes ajout√©es :**\n",
                "5. `cleaned_text` : Texte nettoy√© (lowercase, sans caract√®res sp√©ciaux)\n",
                "6. `lemmatized_text` : Texte lemmatis√© avec spaCy\n",
                "\n",
                "### Utilisation (Partie B)\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "\n",
                "# Charger le dataset enrichi\n",
                "df = pd.read_csv('data_preprocessed.csv')\n",
                "\n",
                "# Acc√©der aux diff√©rentes versions du texte\n",
                "original_texts = df['text']\n",
                "cleaned_texts = df['cleaned_text']\n",
                "lemmatized_texts = df['lemmatized_text']\n",
                "\n",
                "# Acc√©der aux annotations NER originales\n",
                "ner_tags = df['ner_tags']\n",
                "```\n",
                "\n",
                "### Pipeline Appliqu√©\n",
                "\n",
                "1. **Nettoyage** : Lowercase + Suppression caract√®res sp√©ciaux + Normalisation espaces\n",
                "2. **Lemmatization** : spaCy `en_core_web_sm`\n",
                "\n",
                "**Note** : Toutes les colonnes originales sont conserv√©es intactes !"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}