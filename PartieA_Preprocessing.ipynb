{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Représentation Text\n",
                "\n",
                "**Université Paris Cité - Master 2 VMI**\n",
                "**Cours :** IFLCE085 Recherche et extraction sémantique à partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**Équipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## Étape 1 : Setup et Importations\n",
                "Objectif : Configurer l'environnement et importer les librairies nécessaires."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pandas in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (2.0.3)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pandas) (2025.1)\n",
                        "Requirement already satisfied: tzdata>=2022.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pandas) (2025.1)\n",
                        "Requirement already satisfied: numpy>=1.20.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: numpy in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (1.24.4)\n",
                        "Requirement already satisfied: nltk in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (3.9.1)\n",
                        "Requirement already satisfied: click in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
                        "Requirement already satisfied: joblib in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
                        "Requirement already satisfied: tqdm in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: scikit-learn in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (1.3.2)\n",
                        "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
                        "Requirement already satisfied: scipy>=1.5.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
                        "Requirement already satisfied: joblib>=1.1.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
                        "Requirement already satisfied: spacy in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (3.7.6)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
                        "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (8.2.5)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
                        "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (0.12.5)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (4.67.1)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (2.32.3)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (2.10.6)\n",
                        "Requirement already satisfied: jinja2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (3.1.4)\n",
                        "Requirement already satisfied: setuptools in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (75.3.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (24.2)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
                        "Requirement already satisfied: numpy>=1.15.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy) (1.24.4)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
                        "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
                        "  Downloading pydantic_core-2.27.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
                        "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.10)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
                        "Requirement already satisfied: click>=8.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
                        "Requirement already satisfied: rich>=10.11.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from jinja2->spacy) (2.1.5)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
                        "Requirement already satisfied: wrapt in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
                        "Downloading pydantic_core-2.27.2-cp38-cp38-macosx_11_0_arm64.whl (1.8 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: pydantic-core\n",
                        "  Attempting uninstall: pydantic-core\n",
                        "    Found existing installation: pydantic_core 2.20.1\n",
                        "    Uninstalling pydantic_core-2.20.1:\n",
                        "      Successfully uninstalled pydantic_core-2.20.1\n",
                        "Successfully installed pydantic-core-2.27.2\n",
                        "Collecting en-core-web-sm==3.7.1\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
                        "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
                        "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
                        "Requirement already satisfied: jinja2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
                        "Requirement already satisfied: setuptools in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.3.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
                        "Requirement already satisfied: numpy>=1.15.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.27.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.20)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
                        "Requirement already satisfied: click>=8.0.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
                        "Requirement already satisfied: rich>=10.11.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
                        "Requirement already satisfied: wrapt in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /Users/kojack/miniforge3/envs/tf-metal/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
                        "Installing collected packages: en-core-web-sm\n",
                        "Successfully installed en-core-web-sm-3.7.1\n",
                        "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n",
                        "Environnement configuré avec succès.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "# Installation des dépendances de base\n",
                "!{sys.executable} -m pip install pandas\n",
                "!{sys.executable} -m pip install numpy\n",
                "!{sys.executable} -m pip install nltk\n",
                "!{sys.executable} -m pip install scikit-learn\n",
                "!{sys.executable} -m pip install spacy\n",
                "\n",
                "# Téléchargement du modèle spaCy anglais\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pickle\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from scipy.sparse import save_npz\n",
                "\n",
                "# Téléchargement des ressources NLTK (version mise à jour)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du modèle spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"Environnement configuré avec succès.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 2 : Chargement et Exploration des Données (SciREX)\n",
                "Objectif : Télécharger (si nécessaire) et charger le dataset SciREX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Les données sont déjà présentes.\n",
                        "Chargement des données...\n",
                        "Nombre de documents Train : 306\n",
                        "Nombre de documents Dev   : 66\n",
                        "Nombre de documents Test  : 66\n"
                    ]
                }
            ],
            "source": [
                "import urllib.request\n",
                "import tarfile\n",
                "\n",
                "DATA_DIR = \"release_data\"\n",
                "DATA_URL = \"https://github.com/allenai/SciREX/raw/master/scirex_dataset/release_data.tar.gz\"\n",
                "TAR_FILE = \"release_data.tar.gz\"\n",
                "\n",
                "def download_and_extract_data():\n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(\"Dossier de données non trouvé. Téléchargement en cours...\")\n",
                "        try:\n",
                "            urllib.request.urlretrieve(DATA_URL, TAR_FILE)\n",
                "            print(\"Téléchargement terminé. Extraction...\")\n",
                "            with tarfile.open(TAR_FILE, \"r:gz\") as tar:\n",
                "                tar.extractall()\n",
                "            print(\"Extraction terminée.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Erreur lors du téléchargement/extraction : {e}\")\n",
                "    else:\n",
                "        print(\"Les données sont déjà présentes.\")\n",
                "\n",
                "download_and_extract_data()\n",
                "\n",
                "FILES = {\n",
                "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
                "    \"dev\": os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
                "    \"test\": os.path.join(DATA_DIR, \"test.jsonl\")\n",
                "}\n",
                "\n",
                "def load_jsonl(file_path):\n",
                "    \"\"\"Charge un fichier JSONL dans une liste de dictionnaires.\"\"\"\n",
                "    data = []\n",
                "    if os.path.exists(file_path):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                data.append(json.loads(line))\n",
                "    else:\n",
                "        print(f\"Attention : Fichier {file_path} introuvable.\")\n",
                "    return data\n",
                "\n",
                "# Chargement des données\n",
                "print(\"Chargement des données...\")\n",
                "train_data = load_jsonl(FILES[\"train\"])\n",
                "dev_data = load_jsonl(FILES[\"dev\"])\n",
                "test_data = load_jsonl(FILES[\"test\"])\n",
                "\n",
                "print(f\"Nombre de documents Train : {len(train_data)}\")\n",
                "print(f\"Nombre de documents Dev   : {len(dev_data)}\")\n",
                "print(f\"Nombre de documents Test  : {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Clés disponibles dans un document :\n",
                        "['coref', 'coref_non_salient', 'doc_id', 'method_subrelations', 'n_ary_relations', 'ner', 'sections', 'sentences', 'words']\n",
                        "\n",
                        "Exemple de contenu (champs principaux) :\n",
                        "ID: 000f90380d768a85e2316225854fc377c079b5c4\n",
                        "Début du texte (50 premiers mots) : Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an accurate understanding of the surrounding scene is crucial to navigation and action planning . Current state - of - the -...\n"
                    ]
                }
            ],
            "source": [
                "# Exploration d'un document type\n",
                "if train_data:\n",
                "    doc_example = train_data[0]\n",
                "    print(\"\\nClés disponibles dans un document :\")\n",
                "    print(list(doc_example.keys()))\n",
                "\n",
                "    print(\"\\nExemple de contenu (champs principaux) :\")\n",
                "    print(f\"ID: {doc_example.get('doc_id')}\")\n",
                "    if 'words' in doc_example:\n",
                "        print(f\"Début du texte (50 premiers mots) : {' '.join(doc_example['words'][:50])}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 3 : Nettoyage et Normalisation\n",
                "Objectif : Nettoyer le texte (lowercase, suppression caractères spéciaux, espaces multiples)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonction clean_text() définie.\n"
                    ]
                }
            ],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caractères spéciaux, espaces multiples.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à nettoyer\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte nettoyé\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caractères spéciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "print(\"Fonction clean_text() définie.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte original (200 premiers caractères) :\n",
                        "Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an acc\n",
                        "\n",
                        "Texte nettoyé (200 premiers caractères) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate \n",
                        "\n",
                        "Longueur originale : 36348 caractères\n",
                        "Longueur nettoyée  : 34301 caractères\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'words' in doc_example:\n",
                "    raw_text = ' '.join(doc_example['words'])\n",
                "    cleaned_text = clean_text(raw_text)\n",
                "    \n",
                "    print(\"Texte original (200 premiers caractères) :\")\n",
                "    print(raw_text[:200])\n",
                "    print(\"\\nTexte nettoyé (200 premiers caractères) :\")\n",
                "    print(cleaned_text[:200])\n",
                "    print(f\"\\nLongueur originale : {len(raw_text)} caractères\")\n",
                "    print(f\"Longueur nettoyée  : {len(cleaned_text)} caractères\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application du nettoyage sur tous les documents...\n",
                        "Nettoyage terminé pour 306 docs train, 66 docs dev, 66 docs test.\n",
                        "\n",
                        "Exemple de texte nettoyé (doc 0) : full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of...\n"
                    ]
                }
            ],
            "source": [
                "# Application du nettoyage sur tous les documents\n",
                "print(\"Application du nettoyage sur tous les documents...\")\n",
                "\n",
                "for doc in train_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in dev_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in test_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "print(f\"Nettoyage terminé pour {len(train_data)} docs train, {len(dev_data)} docs dev, {len(test_data)} docs test.\")\n",
                "print(f\"\\nExemple de texte nettoyé (doc 0) : {train_data[0]['cleaned_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 4 : Tokenization, POS Tagging et Lemmatization\n",
                "Objectif : Tokeniser, identifier les parties du discours (POS) et lemmatiser les textes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonctions tokenize_and_pos() et lemmatize_text() définies.\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_and_pos(text):\n",
                "    \"\"\"\n",
                "    Tokenise le texte et effectue le POS tagging avec NLTK.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à tokeniser\n",
                "    \n",
                "    Returns:\n",
                "        list: Liste de tuples (token, pos_tag)\n",
                "    \"\"\"\n",
                "    tokens = word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    return pos_tags\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte à lemmatiser\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte lemmatisé\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "print(\"Fonctions tokenize_and_pos() et lemmatize_text() définies.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte nettoyé (extrait) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate understanding of the surrounding scene is crucial to navigation and action planning current state of the art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole while these networks exhibit outstanding recognition perf\n",
                        "\n",
                        "--- Tokenization + POS Tagging ---\n",
                        "Nombre de tokens : 70\n",
                        "Premiers 10 tokens avec POS : [('full', 'JJ'), ('resolution', 'NN'), ('residual', 'JJ'), ('networks', 'NNS'), ('for', 'IN'), ('semantic', 'JJ'), ('segmentation', 'NN'), ('in', 'IN'), ('street', 'NN'), ('scenes', 'NNS')]\n",
                        "\n",
                        "--- Lemmatization ---\n",
                        "Texte lemmatisé (extrait) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of modern autonomous driving system as an accurate und...\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'cleaned_text' in train_data[0]:\n",
                "    sample_text = train_data[0]['cleaned_text'][:500]  # Premier 500 caractères\n",
                "    \n",
                "    print(\"Texte nettoyé (extrait) :\")\n",
                "    print(sample_text)\n",
                "    \n",
                "    print(\"\\n--- Tokenization + POS Tagging ---\")\n",
                "    pos_tags = tokenize_and_pos(sample_text)\n",
                "    print(f\"Nombre de tokens : {len(pos_tags)}\")\n",
                "    print(f\"Premiers 10 tokens avec POS : {pos_tags[:10]}\")\n",
                "    \n",
                "    print(\"\\n--- Lemmatization ---\")\n",
                "    lemmatized = lemmatize_text(sample_text)\n",
                "    print(f\"Texte lemmatisé (extrait) : {lemmatized[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cela peut prendre quelques minutes...\n",
                        "\n",
                        "Application de la lemmatization sur 306 documents TRAIN...\n",
                        "  Traité 50/306 documents...\n",
                        "  Traité 100/306 documents...\n",
                        "  Traité 150/306 documents...\n",
                        "  Traité 200/306 documents...\n",
                        "  Traité 250/306 documents...\n",
                        "  Traité 300/306 documents...\n",
                        "Lemmatization terminée pour TRAIN.\n",
                        "\n",
                        "Application de la lemmatization sur 66 documents DEV...\n",
                        "  Traité 50/66 documents...\n",
                        "Lemmatization terminée pour DEV.\n",
                        "\n",
                        "Application de la lemmatization sur 66 documents TEST...\n",
                        "  Traité 50/66 documents...\n",
                        "Lemmatization terminée pour TEST.\n",
                        "\n",
                        "Exemple de texte lemmatisé (doc 0 train) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of m...\n"
                    ]
                }
            ],
            "source": [
                "# Application de la lemmatization sur TOUS les documents (train, dev, test)\n",
                "def process_dataset_lemmatization(dataset, name):\n",
                "    print(f\"Application de la lemmatization sur {len(dataset)} documents {name}...\")\n",
                "    for i, doc in enumerate(dataset):\n",
                "        if 'cleaned_text' in doc:\n",
                "            doc['lemmatized_text'] = lemmatize_text(doc['cleaned_text'])\n",
                "        if (i + 1) % 50 == 0:\n",
                "            print(f\"  Traité {i + 1}/{len(dataset)} documents...\")\n",
                "    print(f\"Lemmatization terminée pour {name}.\\n\")\n",
                "\n",
                "print(\"Cela peut prendre quelques minutes...\\n\")\n",
                "process_dataset_lemmatization(train_data, \"TRAIN\")\n",
                "process_dataset_lemmatization(dev_data, \"DEV\")\n",
                "process_dataset_lemmatization(test_data, \"TEST\")\n",
                "\n",
                "print(f\"Exemple de texte lemmatisé (doc 0 train) : {train_data[0]['lemmatized_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 5 : Représentation Vectorielle TF-IDF\n",
                "Objectif : Créer une représentation vectorielle des textes avec TF-IDF sur les textes lemmatisés."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calcul de la matrice TF-IDF sur TRAIN (fit_transform)...\n",
                        "Transformation des ensembles DEV et TEST (transform)...\n",
                        "\n",
                        "Matrices TF-IDF créées :\n",
                        "  TRAIN : (306, 5000) (Densité : 0.2372)\n",
                        "  DEV   : (66, 5000) (Densité : 0.2329)\n",
                        "  TEST  : (66, 5000) (Densité : 0.2535)\n"
                    ]
                }
            ],
            "source": [
                "# Préparation des textes pour TF-IDF (utilisation des textes LEMMATISÉS)\n",
                "train_texts = [doc['lemmatized_text'] for doc in train_data if 'lemmatized_text' in doc]\n",
                "dev_texts = [doc['lemmatized_text'] for doc in dev_data if 'lemmatized_text' in doc]\n",
                "test_texts = [doc['lemmatized_text'] for doc in test_data if 'lemmatized_text' in doc]\n",
                "\n",
                "# Création du vectoriseur TF-IDF\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_features=5000,  # Limite à 5000 features les plus importantes\n",
                "    min_df=2,           # Ignore les termes qui apparaissent dans moins de 2 documents\n",
                "    max_df=0.8,         # Ignore les termes qui apparaissent dans plus de 80% des documents\n",
                "    ngram_range=(1, 2)  # Unigrammes et bigrammes\n",
                ")\n",
                "\n",
                "# Calcul de la matrice TF-IDF sur TRAIN (fit_transform)\n",
                "print(\"Calcul de la matrice TF-IDF sur TRAIN (fit_transform)...\")\n",
                "tfidf_matrix = tfidf_vectorizer.fit_transform(train_texts)\n",
                "\n",
                "# Transformation des ensembles DEV et TEST (transform uniquement)\n",
                "print(\"Transformation des ensembles DEV et TEST (transform)...\")\n",
                "tfidf_matrix_dev = tfidf_vectorizer.transform(dev_texts)\n",
                "tfidf_matrix_test = tfidf_vectorizer.transform(test_texts)\n",
                "\n",
                "print(f\"\\nMatrices TF-IDF créées :\")\n",
                "print(f\"  TRAIN : {tfidf_matrix.shape} (Densité : {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f})\")\n",
                "print(f\"  DEV   : {tfidf_matrix_dev.shape} (Densité : {tfidf_matrix_dev.nnz / (tfidf_matrix_dev.shape[0] * tfidf_matrix_dev.shape[1]):.4f})\")\n",
                "print(f\"  TEST  : {tfidf_matrix_test.shape} (Densité : {tfidf_matrix_test.nnz / (tfidf_matrix_test.shape[0] * tfidf_matrix_test.shape[1]):.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 10 features TF-IDF pour le document 0 :\n",
                        "  stream: 0.3775\n",
                        "  residual: 0.2422\n",
                        "  cityscape: 0.2304\n",
                        "  resolution: 0.1873\n",
                        "  segmentation: 0.1832\n",
                        "  image: 0.1744\n",
                        "  reference reference: 0.1668\n",
                        "  resnet: 0.1544\n",
                        "  boundary: 0.1355\n",
                        "  pooling: 0.1243\n"
                    ]
                }
            ],
            "source": [
                "# Affichage des top features pour le premier document\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "doc_0_vector = tfidf_matrix[0].toarray()[0]\n",
                "top_indices = doc_0_vector.argsort()[-10:][::-1]\n",
                "\n",
                "print(\"Top 10 features TF-IDF pour le document 0 :\")\n",
                "for idx in top_indices:\n",
                "    print(f\"  {feature_names[idx]}: {doc_0_vector[idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Étape 6 : Export et Sauvegarde des Résultats\n",
                "Objectif : Sauvegarder tous les résultats pour la Partie B."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dossier de sortie créé : preprocessed_data/\n"
                    ]
                }
            ],
            "source": [
                "# Création du dossier de sortie\n",
                "OUTPUT_DIR = \"preprocessed_data\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Dossier de sortie créé : {OUTPUT_DIR}/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 - Export des textes prétraités (CSV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Fichier sauvegardé : preprocessed_data/train_preprocessed.csv\n",
                        "   Colonnes : ['doc_id', 'raw_text', 'cleaned_text', 'lemmatized_text']\n",
                        "   Nombre de lignes : 306\n",
                        "✅ Fichier sauvegardé : preprocessed_data/dev_preprocessed.csv\n",
                        "   Colonnes : ['doc_id', 'raw_text', 'cleaned_text', 'lemmatized_text']\n",
                        "   Nombre de lignes : 66\n",
                        "✅ Fichier sauvegardé : preprocessed_data/test_preprocessed.csv\n",
                        "   Colonnes : ['doc_id', 'raw_text', 'cleaned_text', 'lemmatized_text']\n",
                        "   Nombre de lignes : 66\n"
                    ]
                }
            ],
            "source": [
                "def export_to_csv(dataset, filename):\n",
                "    df = pd.DataFrame([\n",
                "        {\n",
                "            'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "            'raw_text': ' '.join(doc.get('words', [])),\n",
                "            'cleaned_text': doc.get('cleaned_text', ''),\n",
                "            'lemmatized_text': doc.get('lemmatized_text', '')\n",
                "        }\n",
                "        for i, doc in enumerate(dataset)\n",
                "    ])\n",
                "    df.to_csv(os.path.join(OUTPUT_DIR, filename), index=False, encoding='utf-8')\n",
                "    print(f\"✅ Fichier sauvegardé : {OUTPUT_DIR}/{filename}\")\n",
                "    print(f\"   Colonnes : {list(df.columns)}\")\n",
                "    print(f\"   Nombre de lignes : {len(df)}\")\n",
                "\n",
                "export_to_csv(train_data, 'train_preprocessed.csv')\n",
                "export_to_csv(dev_data, 'dev_preprocessed.csv')\n",
                "export_to_csv(test_data, 'test_preprocessed.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 - Export de la matrice TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Matrice TF-IDF TRAIN sauvegardée : preprocessed_data/tfidf_matrix.npz\n",
                        "✅ Matrice TF-IDF DEV sauvegardée   : preprocessed_data/tfidf_matrix_dev.npz\n",
                        "✅ Matrice TF-IDF TEST sauvegardée  : preprocessed_data/tfidf_matrix_test.npz\n",
                        "✅ Vectoriseur TF-IDF sauvegardé : preprocessed_data/tfidf_vectorizer.pkl\n",
                        "✅ Noms des features sauvegardés : preprocessed_data/tfidf_feature_names.npy\n"
                    ]
                }
            ],
            "source": [
                "# Sauvegarde des matrices TF-IDF (format sparse)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix.npz'), tfidf_matrix)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_dev.npz'), tfidf_matrix_dev)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_test.npz'), tfidf_matrix_test)\n",
                "\n",
                "print(f\"✅ Matrice TF-IDF TRAIN sauvegardée : {OUTPUT_DIR}/tfidf_matrix.npz\")\n",
                "print(f\"✅ Matrice TF-IDF DEV sauvegardée   : {OUTPUT_DIR}/tfidf_matrix_dev.npz\")\n",
                "print(f\"✅ Matrice TF-IDF TEST sauvegardée  : {OUTPUT_DIR}/tfidf_matrix_test.npz\")\n",
                "\n",
                "# Sauvegarde du vectoriseur (pour réutilisation)\n",
                "with open(os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
                "    pickle.dump(tfidf_vectorizer, f)\n",
                "print(f\"✅ Vectoriseur TF-IDF sauvegardé : {OUTPUT_DIR}/tfidf_vectorizer.pkl\")\n",
                "\n",
                "# Sauvegarde des noms de features\n",
                "np.save(os.path.join(OUTPUT_DIR, 'tfidf_feature_names.npy'), feature_names)\n",
                "print(f\"✅ Noms des features sauvegardés : {OUTPUT_DIR}/tfidf_feature_names.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 - Export du dictionnaire de correspondance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Dictionnaire de correspondance sauvegardé : preprocessed_data/correspondence_dict.json\n"
                    ]
                }
            ],
            "source": [
                "# Création d'un dictionnaire de correspondance complet\n",
                "correspondence_dict = {\n",
                "    'metadata': {\n",
                "        'n_documents_train': len(train_data),\n",
                "        'n_documents_dev': len(dev_data),\n",
                "        'n_documents_test': len(test_data),\n",
                "        'tfidf_shape_train': tfidf_matrix.shape,\n",
                "        'tfidf_shape_dev': tfidf_matrix_dev.shape,\n",
                "        'tfidf_shape_test': tfidf_matrix_test.shape,\n",
                "        'n_features': len(feature_names),\n",
                "        'preprocessing_steps': [\n",
                "            '1. Lowercase',\n",
                "            '2. Suppression caractères spéciaux',\n",
                "            '3. Normalisation espaces',\n",
                "            '4. Tokenization (NLTK)',\n",
                "            '5. POS Tagging (NLTK)',\n",
                "            '6. Lemmatization (spaCy)',\n",
                "            '7. TF-IDF sur textes lemmatisés (max_features=5000, ngram_range=(1,2))'\n",
                "        ]\n",
                "    },\n",
                "    'documents_train_sample': [\n",
                "        {\n",
                "            'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "            'raw_text_preview': ' '.join(doc.get('words', []))[:200],\n",
                "            'cleaned_text_preview': doc.get('cleaned_text', '')[:200],\n",
                "            'lemmatized_text_preview': doc.get('lemmatized_text', '')[:200],\n",
                "            'tfidf_vector_index': i\n",
                "        }\n",
                "        for i, doc in enumerate(train_data[:5])\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Sauvegarde en JSON\n",
                "with open(os.path.join(OUTPUT_DIR, 'correspondence_dict.json'), 'w', encoding='utf-8') as f:\n",
                "    json.dump(correspondence_dict, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"✅ Dictionnaire de correspondance sauvegardé : {OUTPUT_DIR}/correspondence_dict.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 - Résumé des fichiers exportés"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "RÉSUMÉ DES FICHIERS EXPORTÉS\n",
                        "============================================================\n",
                        "📄 correspondence_dict.json            (0.00 MB)\n",
                        "📄 dev_preprocessed.csv                (5.44 MB)\n",
                        "📄 test_preprocessed.csv               (6.13 MB)\n",
                        "📄 tfidf_feature_names.npy             (0.09 MB)\n",
                        "📄 tfidf_matrix.npz                    (1.96 MB)\n",
                        "📄 tfidf_matrix_dev.npz                (0.41 MB)\n",
                        "📄 tfidf_matrix_test.npz               (0.44 MB)\n",
                        "📄 tfidf_vectorizer.pkl                (5.29 MB)\n",
                        "📄 train_preprocessed.csv              (25.73 MB)\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RÉSUMÉ DES FICHIERS EXPORTÉS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    if os.path.isfile(filepath):\n",
                "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
                "        print(f\"📄 {filename:35s} ({size_mb:.2f} MB)\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Résumé Technique - Partie A\n",
                "\n",
                "### Pipeline de Preprocessing Complet\n",
                "\n",
                "1. **Nettoyage** :\n",
                "   - Conversion en lowercase\n",
                "   - Suppression des caractères spéciaux (garde uniquement lettres, chiffres, espaces)\n",
                "   - Normalisation des espaces multiples\n",
                "\n",
                "2. **Tokenization** :\n",
                "   - Tokenization avec NLTK (`word_tokenize`)\n",
                "\n",
                "3. **POS Tagging** :\n",
                "   - POS tagging avec NLTK (`pos_tag`)\n",
                "\n",
                "4. **Lemmatization** :\n",
                "   - Lemmatization avec spaCy (`en_core_web_sm`)\n",
                "   - **Appliqué sur TOUS les documents (Train, Dev, Test)**\n",
                "\n",
                "5. **Représentation TF-IDF** :\n",
                "   - Vectorisation avec scikit-learn `TfidfVectorizer`\n",
                "   - **Apprentissage (Fit)** sur le TRAIN set uniquement\n",
                "   - **Transformation** appliquée sur TRAIN, DEV et TEST\n",
                "   - Paramètres : `max_features=5000`, `min_df=2`, `max_df=0.8`, `ngram_range=(1,2)`\n",
                "   - Matrice résultante : N documents × 5000 features\n",
                "\n",
                "---\n",
                "\n",
                "### Fichiers Exportés pour la Partie B\n",
                "\n",
                "| Fichier | Description | Format |\n",
                "|---------|-------------|--------|\n",
                "| `train_preprocessed.csv` | Textes TRAIN prétraités | CSV |\n",
                "| `dev_preprocessed.csv` | Textes DEV prétraités | CSV |\n",
                "| `test_preprocessed.csv` | Textes TEST prétraités | CSV |\n",
                "| `tfidf_matrix.npz` | Matrice TF-IDF TRAIN (sparse) | NumPy compressed |\n",
                "| `tfidf_matrix_dev.npz` | Matrice TF-IDF DEV (sparse) | NumPy compressed |\n",
                "| `tfidf_matrix_test.npz` | Matrice TF-IDF TEST (sparse) | NumPy compressed |\n",
                "| `tfidf_vectorizer.pkl` | Vectoriseur TF-IDF entraîné | Pickle |\n",
                "| `tfidf_feature_names.npy` | Noms des 5000 features TF-IDF | NumPy |\n",
                "| `correspondence_dict.json` | Métadonnées et correspondances | JSON |\n",
                "\n",
                "---\n",
                "\n",
                "### Comment Charger les Données (Partie B)\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "from scipy.sparse import load_npz\n",
                "\n",
                "# Charger les textes prétraités\n",
                "df_train = pd.read_csv('preprocessed_data/train_preprocessed.csv')\n",
                "df_dev = pd.read_csv('preprocessed_data/dev_preprocessed.csv')\n",
                "\n",
                "# Charger les matrices TF-IDF\n",
                "tfidf_train = load_npz('preprocessed_data/tfidf_matrix.npz')\n",
                "tfidf_dev = load_npz('preprocessed_data/tfidf_matrix_dev.npz')\n",
                "\n",
                "# Charger le vectoriseur (pour transformer de nouveaux textes)\n",
                "with open('preprocessed_data/tfidf_vectorizer.pkl', 'rb') as f:\n",
                "    vectorizer = pickle.load(f)\n",
                "\n",
                "# Charger les noms de features\n",
                "feature_names = np.load('preprocessed_data/tfidf_feature_names.npy')\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### Statistiques Finales\n",
                "\n",
                "- **Documents train** : 306\n",
                "- **Documents dev** : 66\n",
                "- **Documents test** : 66\n",
                "- **Features TF-IDF** : 5000\n",
                "- **Pipeline** : Nettoyage → Tokenization → POS → Lemmatization → TF-IDF\n",
                "\n",
                "---\n",
                "\n",
                "**Note** : Le pipeline complet garantit que TF-IDF est calculé sur des textes entièrement prétraités (lemmatisés), ce qui améliore la qualité des représentations vectorielles."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tf-metal",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
