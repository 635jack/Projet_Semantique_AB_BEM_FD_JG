{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Repr√©sentation Text\n",
                "\n",
                "**Universit√© Paris Cit√© - Master 2 VMI**\n",
                "**Cours :** IFLCE085 Recherche et extraction s√©mantique √† partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**√âquipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## √âtape 1 : Setup et Importations\n",
                "Objectif : Configurer l'environnement et importer les librairies n√©cessaires."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
                        "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
                        "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
                        "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
                        "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
                        "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
                        "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
                        "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
                        "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
                        "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
                        "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
                        "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
                        "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
                        "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
                        "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
                        "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
                        "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
                        "Collecting en-core-web-sm==3.8.0\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n",
                        "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
                        "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
                        "order to load all the package's dependencies. You can do this by selecting the\n",
                        "'Restart kernel' or 'Restart runtime' option.\n",
                        "Environnement configur√© avec succ√®s.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "# Installation des d√©pendances de base\n",
                "!{sys.executable} -m pip install pandas\n",
                "!{sys.executable} -m pip install numpy\n",
                "!{sys.executable} -m pip install nltk\n",
                "!{sys.executable} -m pip install scikit-learn\n",
                "!{sys.executable} -m pip install spacy\n",
                "\n",
                "# T√©l√©chargement du mod√®le spaCy anglais\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pickle\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from scipy.sparse import save_npz\n",
                "\n",
                "# T√©l√©chargement des ressources NLTK (version mise √† jour)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du mod√®le spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"Environnement configur√© avec succ√®s.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 2 : Chargement et Exploration des Donn√©es (SciREX)\n",
                "Objectif : T√©l√©charger (si n√©cessaire) et charger le dataset SciREX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Les donn√©es sont d√©j√† pr√©sentes.\n",
                        "Chargement des donn√©es...\n",
                        "Nombre de documents Train : 306\n",
                        "Nombre de documents Dev   : 66\n",
                        "Nombre de documents Test  : 66\n"
                    ]
                }
            ],
            "source": [
                "import urllib.request\n",
                "import tarfile\n",
                "\n",
                "DATA_DIR = \"release_data\"\n",
                "DATA_URL = \"https://github.com/allenai/SciREX/raw/master/scirex_dataset/release_data.tar.gz\"\n",
                "TAR_FILE = \"release_data.tar.gz\"\n",
                "\n",
                "def download_and_extract_data():\n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(\"Dossier de donn√©es non trouv√©. T√©l√©chargement en cours...\")\n",
                "        try:\n",
                "            urllib.request.urlretrieve(DATA_URL, TAR_FILE)\n",
                "            print(\"T√©l√©chargement termin√©. Extraction...\")\n",
                "            with tarfile.open(TAR_FILE, \"r:gz\") as tar:\n",
                "                tar.extractall()\n",
                "            print(\"Extraction termin√©e.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Erreur lors du t√©l√©chargement/extraction : {e}\")\n",
                "    else:\n",
                "        print(\"Les donn√©es sont d√©j√† pr√©sentes.\")\n",
                "\n",
                "download_and_extract_data()\n",
                "\n",
                "FILES = {\n",
                "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
                "    \"dev\": os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
                "    \"test\": os.path.join(DATA_DIR, \"test.jsonl\")\n",
                "}\n",
                "\n",
                "def load_jsonl(file_path):\n",
                "    \"\"\"Charge un fichier JSONL dans une liste de dictionnaires.\"\"\"\n",
                "    data = []\n",
                "    if os.path.exists(file_path):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                data.append(json.loads(line))\n",
                "    else:\n",
                "        print(f\"Attention : Fichier {file_path} introuvable.\")\n",
                "    return data\n",
                "\n",
                "# Chargement des donn√©es\n",
                "print(\"Chargement des donn√©es...\")\n",
                "train_data = load_jsonl(FILES[\"train\"])\n",
                "dev_data = load_jsonl(FILES[\"dev\"])\n",
                "test_data = load_jsonl(FILES[\"test\"])\n",
                "\n",
                "print(f\"Nombre de documents Train : {len(train_data)}\")\n",
                "print(f\"Nombre de documents Dev   : {len(dev_data)}\")\n",
                "print(f\"Nombre de documents Test  : {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Cl√©s disponibles dans un document :\n",
                        "['coref', 'coref_non_salient', 'doc_id', 'method_subrelations', 'n_ary_relations', 'ner', 'sections', 'sentences', 'words']\n",
                        "\n",
                        "Exemple de contenu (champs principaux) :\n",
                        "ID: 000f90380d768a85e2316225854fc377c079b5c4\n",
                        "D√©but du texte (50 premiers mots) : Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an accurate understanding of the surrounding scene is crucial to navigation and action planning . Current state - of - the -...\n"
                    ]
                }
            ],
            "source": [
                "# Exploration d'un document type\n",
                "if train_data:\n",
                "    doc_example = train_data[0]\n",
                "    print(\"\\nCl√©s disponibles dans un document :\")\n",
                "    print(list(doc_example.keys()))\n",
                "\n",
                "    print(\"\\nExemple de contenu (champs principaux) :\")\n",
                "    print(f\"ID: {doc_example.get('doc_id')}\")\n",
                "    if 'words' in doc_example:\n",
                "        print(f\"D√©but du texte (50 premiers mots) : {' '.join(doc_example['words'][:50])}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 3 : Nettoyage et Normalisation\n",
                "Objectif : Nettoyer le texte (lowercase, suppression caract√®res sp√©ciaux, espaces multiples)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonction clean_text() d√©finie.\n"
                    ]
                }
            ],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caract√®res sp√©ciaux, espaces multiples.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† nettoyer\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte nettoy√©\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caract√®res sp√©ciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "print(\"Fonction clean_text() d√©finie.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte original (200 premiers caract√®res) :\n",
                        "Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an acc\n",
                        "\n",
                        "Texte nettoy√© (200 premiers caract√®res) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate \n",
                        "\n",
                        "Longueur originale : 36348 caract√®res\n",
                        "Longueur nettoy√©e  : 34301 caract√®res\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'words' in doc_example:\n",
                "    raw_text = ' '.join(doc_example['words'])\n",
                "    cleaned_text = clean_text(raw_text)\n",
                "    \n",
                "    print(\"Texte original (200 premiers caract√®res) :\")\n",
                "    print(raw_text[:200])\n",
                "    print(\"\\nTexte nettoy√© (200 premiers caract√®res) :\")\n",
                "    print(cleaned_text[:200])\n",
                "    print(f\"\\nLongueur originale : {len(raw_text)} caract√®res\")\n",
                "    print(f\"Longueur nettoy√©e  : {len(cleaned_text)} caract√®res\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application du nettoyage sur tous les documents...\n",
                        "Nettoyage termin√© pour 306 docs train, 66 docs dev, 66 docs test.\n",
                        "\n",
                        "Exemple de texte nettoy√© (doc 0) : full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of...\n"
                    ]
                }
            ],
            "source": [
                "# Application du nettoyage sur tous les documents\n",
                "print(\"Application du nettoyage sur tous les documents...\")\n",
                "\n",
                "for doc in train_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in dev_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in test_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "print(f\"Nettoyage termin√© pour {len(train_data)} docs train, {len(dev_data)} docs dev, {len(test_data)} docs test.\")\n",
                "print(f\"\\nExemple de texte nettoy√© (doc 0) : {train_data[0]['cleaned_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 4 : Tokenization, POS Tagging et Lemmatization\n",
                "Objectif : Tokeniser, identifier les parties du discours (POS) et lemmatiser les textes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fonctions tokenize_and_pos() et lemmatize_text() d√©finies.\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_and_pos(text):\n",
                "    \"\"\"\n",
                "    Tokenise le texte et effectue le POS tagging avec NLTK.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† tokeniser\n",
                "    \n",
                "    Returns:\n",
                "        list: Liste de tuples (token, pos_tag)\n",
                "    \"\"\"\n",
                "    tokens = word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    return pos_tags\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† lemmatiser\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte lemmatis√©\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "print(\"Fonctions tokenize_and_pos() et lemmatize_text() d√©finies.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Texte nettoy√© (extrait) :\n",
                        "full resolution residual networks for semantic segmentation in street scenes section abstract semantic image segmentation is an essential component of modern autonomous driving systems as an accurate understanding of the surrounding scene is crucial to navigation and action planning current state of the art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole while these networks exhibit outstanding recognition perf\n",
                        "\n",
                        "--- Tokenization + POS Tagging ---\n",
                        "Nombre de tokens : 70\n",
                        "Premiers 10 tokens avec POS : [('full', 'JJ'), ('resolution', 'NN'), ('residual', 'JJ'), ('networks', 'NNS'), ('for', 'IN'), ('semantic', 'JJ'), ('segmentation', 'NN'), ('in', 'IN'), ('street', 'NN'), ('scenes', 'NNS')]\n",
                        "\n",
                        "--- Lemmatization ---\n",
                        "Texte lemmatis√© (extrait) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of modern autonomous driving system as an accurate und...\n"
                    ]
                }
            ],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'cleaned_text' in train_data[0]:\n",
                "    sample_text = train_data[0]['cleaned_text'][:500]  # Premier 500 caract√®res\n",
                "    \n",
                "    print(\"Texte nettoy√© (extrait) :\")\n",
                "    print(sample_text)\n",
                "    \n",
                "    print(\"\\n--- Tokenization + POS Tagging ---\")\n",
                "    pos_tags = tokenize_and_pos(sample_text)\n",
                "    print(f\"Nombre de tokens : {len(pos_tags)}\")\n",
                "    print(f\"Premiers 10 tokens avec POS : {pos_tags[:10]}\")\n",
                "    \n",
                "    print(\"\\n--- Lemmatization ---\")\n",
                "    lemmatized = lemmatize_text(sample_text)\n",
                "    print(f\"Texte lemmatis√© (extrait) : {lemmatized[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Application de la lemmatization sur 306 documents train...\n",
                        "Cela peut prendre quelques minutes...\n",
                        "\n",
                        "  Trait√© 50/306 documents...\n",
                        "  Trait√© 100/306 documents...\n",
                        "  Trait√© 150/306 documents...\n",
                        "  Trait√© 200/306 documents...\n",
                        "  Trait√© 250/306 documents...\n",
                        "  Trait√© 300/306 documents...\n",
                        "\n",
                        "Lemmatization termin√©e pour 306 documents.\n",
                        "Exemple de texte lemmatis√© (doc 0) : full resolution residual network for semantic segmentation in street scene section abstract semantic image segmentation be an essential component of m...\n"
                    ]
                }
            ],
            "source": [
                "# Application de la lemmatization sur TOUS les documents train\n",
                "print(f\"Application de la lemmatization sur {len(train_data)} documents train...\")\n",
                "print(\"Cela peut prendre quelques minutes...\\n\")\n",
                "\n",
                "for i, doc in enumerate(train_data):\n",
                "    if 'cleaned_text' in doc:\n",
                "        doc['lemmatized_text'] = lemmatize_text(doc['cleaned_text'])\n",
                "    if (i + 1) % 50 == 0:\n",
                "        print(f\"  Trait√© {i + 1}/{len(train_data)} documents...\")\n",
                "\n",
                "print(f\"\\nLemmatization termin√©e pour {len(train_data)} documents.\")\n",
                "print(f\"Exemple de texte lemmatis√© (doc 0) : {train_data[0]['lemmatized_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 5 : Repr√©sentation Vectorielle TF-IDF\n",
                "Objectif : Cr√©er une repr√©sentation vectorielle des textes avec TF-IDF sur les textes lemmatis√©s."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calcul de la matrice TF-IDF sur les textes lemmatis√©s...\n",
                        "\n",
                        "Matrice TF-IDF cr√©√©e :\n",
                        "  Forme : (306, 5000)\n",
                        "  Nombre de documents : 306\n",
                        "  Nombre de features : 5000\n",
                        "  Densit√© : 0.2372\n"
                    ]
                }
            ],
            "source": [
                "# Pr√©paration des textes pour TF-IDF (utilisation des textes LEMMATIS√âS)\n",
                "train_texts = [doc['lemmatized_text'] for doc in train_data if 'lemmatized_text' in doc]\n",
                "\n",
                "# Cr√©ation du vectoriseur TF-IDF\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_features=5000,  # Limite √† 5000 features les plus importantes\n",
                "    min_df=2,           # Ignore les termes qui apparaissent dans moins de 2 documents\n",
                "    max_df=0.8,         # Ignore les termes qui apparaissent dans plus de 80% des documents\n",
                "    ngram_range=(1, 2)  # Unigrammes et bigrammes\n",
                ")\n",
                "\n",
                "# Calcul de la matrice TF-IDF\n",
                "print(\"Calcul de la matrice TF-IDF sur les textes lemmatis√©s...\")\n",
                "tfidf_matrix = tfidf_vectorizer.fit_transform(train_texts)\n",
                "\n",
                "print(f\"\\nMatrice TF-IDF cr√©√©e :\")\n",
                "print(f\"  Forme : {tfidf_matrix.shape}\")\n",
                "print(f\"  Nombre de documents : {tfidf_matrix.shape[0]}\")\n",
                "print(f\"  Nombre de features : {tfidf_matrix.shape[1]}\")\n",
                "print(f\"  Densit√© : {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 10 features TF-IDF pour le document 0 :\n",
                        "  stream: 0.3741\n",
                        "  residual: 0.2400\n",
                        "  cityscape: 0.2283\n",
                        "  resolution: 0.1856\n",
                        "  segmentation: 0.1815\n",
                        "  image: 0.1728\n",
                        "  reference reference: 0.1653\n",
                        "  resnet: 0.1530\n",
                        "  pool: 0.1446\n",
                        "  boundary: 0.1343\n"
                    ]
                }
            ],
            "source": [
                "# Affichage des top features pour le premier document\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "doc_0_vector = tfidf_matrix[0].toarray()[0]\n",
                "top_indices = doc_0_vector.argsort()[-10:][::-1]\n",
                "\n",
                "print(\"Top 10 features TF-IDF pour le document 0 :\")\n",
                "for idx in top_indices:\n",
                "    print(f\"  {feature_names[idx]}: {doc_0_vector[idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 6 : Export et Sauvegarde des R√©sultats\n",
                "Objectif : Sauvegarder tous les r√©sultats pour la Partie B."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dossier de sortie cr√©√© : preprocessed_data/\n"
                    ]
                }
            ],
            "source": [
                "# Cr√©ation du dossier de sortie\n",
                "OUTPUT_DIR = \"preprocessed_data\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Dossier de sortie cr√©√© : {OUTPUT_DIR}/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 - Export des textes pr√©trait√©s (CSV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Fichier sauvegard√© : preprocessed_data/train_preprocessed.csv\n",
                        "   Colonnes : ['doc_id', 'raw_text', 'cleaned_text', 'lemmatized_text']\n",
                        "   Nombre de lignes : 306\n"
                    ]
                }
            ],
            "source": [
                "# Cr√©ation d'un DataFrame avec toutes les versions du texte\n",
                "train_df = pd.DataFrame([\n",
                "    {\n",
                "        'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "        'raw_text': ' '.join(doc.get('words', [])),\n",
                "        'cleaned_text': doc.get('cleaned_text', ''),\n",
                "        'lemmatized_text': doc.get('lemmatized_text', '')\n",
                "    }\n",
                "    for i, doc in enumerate(train_data)\n",
                "])\n",
                "\n",
                "# Sauvegarde en CSV\n",
                "train_df.to_csv(os.path.join(OUTPUT_DIR, 'train_preprocessed.csv'), index=False, encoding='utf-8')\n",
                "print(f\"‚úÖ Fichier sauvegard√© : {OUTPUT_DIR}/train_preprocessed.csv\")\n",
                "print(f\"   Colonnes : {list(train_df.columns)}\")\n",
                "print(f\"   Nombre de lignes : {len(train_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 - Export de la matrice TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Matrice TF-IDF sauvegard√©e : preprocessed_data/tfidf_matrix.npz\n",
                        "‚úÖ Vectoriseur TF-IDF sauvegard√© : preprocessed_data/tfidf_vectorizer.pkl\n",
                        "‚úÖ Noms des features sauvegard√©s : preprocessed_data/tfidf_feature_names.npy\n"
                    ]
                }
            ],
            "source": [
                "# Sauvegarde de la matrice TF-IDF (format sparse)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix.npz'), tfidf_matrix)\n",
                "print(f\"‚úÖ Matrice TF-IDF sauvegard√©e : {OUTPUT_DIR}/tfidf_matrix.npz\")\n",
                "\n",
                "# Sauvegarde du vectoriseur (pour r√©utilisation)\n",
                "with open(os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
                "    pickle.dump(tfidf_vectorizer, f)\n",
                "print(f\"‚úÖ Vectoriseur TF-IDF sauvegard√© : {OUTPUT_DIR}/tfidf_vectorizer.pkl\")\n",
                "\n",
                "# Sauvegarde des noms de features\n",
                "np.save(os.path.join(OUTPUT_DIR, 'tfidf_feature_names.npy'), feature_names)\n",
                "print(f\"‚úÖ Noms des features sauvegard√©s : {OUTPUT_DIR}/tfidf_feature_names.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 - Export du dictionnaire de correspondance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Dictionnaire de correspondance sauvegard√© : preprocessed_data/correspondence_dict.json\n"
                    ]
                }
            ],
            "source": [
                "# Cr√©ation d'un dictionnaire de correspondance complet\n",
                "correspondence_dict = {\n",
                "    'metadata': {\n",
                "        'n_documents': len(train_data),\n",
                "        'tfidf_shape': tfidf_matrix.shape,\n",
                "        'n_features': len(feature_names),\n",
                "        'preprocessing_steps': [\n",
                "            '1. Lowercase',\n",
                "            '2. Suppression caract√®res sp√©ciaux',\n",
                "            '3. Normalisation espaces',\n",
                "            '4. Tokenization (NLTK)',\n",
                "            '5. POS Tagging (NLTK)',\n",
                "            '6. Lemmatization (spaCy)',\n",
                "            '7. TF-IDF sur textes lemmatis√©s (max_features=5000, ngram_range=(1,2))'\n",
                "        ]\n",
                "    },\n",
                "    'documents': [\n",
                "        {\n",
                "            'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "            'raw_text_preview': ' '.join(doc.get('words', []))[:200],\n",
                "            'cleaned_text_preview': doc.get('cleaned_text', '')[:200],\n",
                "            'lemmatized_text_preview': doc.get('lemmatized_text', '')[:200],\n",
                "            'tfidf_vector_index': i\n",
                "        }\n",
                "        for i, doc in enumerate(train_data[:10])  # Limit√© aux 10 premiers pour la d√©mo\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Sauvegarde en JSON\n",
                "with open(os.path.join(OUTPUT_DIR, 'correspondence_dict.json'), 'w', encoding='utf-8') as f:\n",
                "    json.dump(correspondence_dict, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"‚úÖ Dictionnaire de correspondance sauvegard√© : {OUTPUT_DIR}/correspondence_dict.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 - R√©sum√© des fichiers export√©s"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "R√âSUM√â DES FICHIERS EXPORT√âS\n",
                        "============================================================\n",
                        "üìÑ tfidf_matrix.npz                    (1.97 MB)\n",
                        "üìÑ correspondence_dict.json            (0.01 MB)\n",
                        "üìÑ tfidf_feature_names.npy             (0.06 MB)\n",
                        "üìÑ tfidf_vectorizer.pkl                (0.19 MB)\n",
                        "üìÑ train_preprocessed.csv              (25.73 MB)\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"R√âSUM√â DES FICHIERS EXPORT√âS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for filename in os.listdir(OUTPUT_DIR):\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    if os.path.isfile(filepath):\n",
                "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
                "        print(f\"üìÑ {filename:35s} ({size_mb:.2f} MB)\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## R√©sum√© Technique - Partie A\n",
                "\n",
                "### Pipeline de Preprocessing Complet\n",
                "\n",
                "1. **Nettoyage** :\n",
                "   - Conversion en lowercase\n",
                "   - Suppression des caract√®res sp√©ciaux (garde uniquement lettres, chiffres, espaces)\n",
                "   - Normalisation des espaces multiples\n",
                "\n",
                "2. **Tokenization** :\n",
                "   - Tokenization avec NLTK (`word_tokenize`)\n",
                "\n",
                "3. **POS Tagging** :\n",
                "   - POS tagging avec NLTK (`pos_tag`)\n",
                "\n",
                "4. **Lemmatization** :\n",
                "   - Lemmatization avec spaCy (`en_core_web_sm`)\n",
                "   - **Appliqu√© sur TOUS les 306 documents train**\n",
                "\n",
                "5. **Repr√©sentation TF-IDF** :\n",
                "   - Vectorisation avec scikit-learn `TfidfVectorizer`\n",
                "   - **Calcul√© sur les textes lemmatis√©s** (pipeline complet)\n",
                "   - Param√®tres : `max_features=5000`, `min_df=2`, `max_df=0.8`, `ngram_range=(1,2)`\n",
                "   - Matrice r√©sultante : 306 documents √ó 5000 features\n",
                "\n",
                "---\n",
                "\n",
                "### Fichiers Export√©s pour la Partie B\n",
                "\n",
                "| Fichier | Description | Format |\n",
                "|---------|-------------|--------|\n",
                "| `train_preprocessed.csv` | Textes bruts, nettoy√©s et lemmatis√©s | CSV |\n",
                "| `tfidf_matrix.npz` | Matrice TF-IDF (sparse) | NumPy compressed |\n",
                "| `tfidf_vectorizer.pkl` | Vectoriseur TF-IDF entra√Æn√© | Pickle |\n",
                "| `tfidf_feature_names.npy` | Noms des 5000 features TF-IDF | NumPy |\n",
                "| `correspondence_dict.json` | M√©tadonn√©es et correspondances | JSON |\n",
                "\n",
                "---\n",
                "\n",
                "### Comment Charger les Donn√©es (Partie B)\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "from scipy.sparse import load_npz\n",
                "\n",
                "# Charger les textes pr√©trait√©s\n",
                "df = pd.read_csv('preprocessed_data/train_preprocessed.csv')\n",
                "\n",
                "# Charger la matrice TF-IDF\n",
                "tfidf_matrix = load_npz('preprocessed_data/tfidf_matrix.npz')\n",
                "\n",
                "# Charger le vectoriseur (pour transformer de nouveaux textes)\n",
                "with open('preprocessed_data/tfidf_vectorizer.pkl', 'rb') as f:\n",
                "    vectorizer = pickle.load(f)\n",
                "\n",
                "# Charger les noms de features\n",
                "feature_names = np.load('preprocessed_data/tfidf_feature_names.npy')\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### Statistiques Finales\n",
                "\n",
                "- **Documents train** : 306 (tous lemmatis√©s)\n",
                "- **Documents dev** : 66\n",
                "- **Documents test** : 66\n",
                "- **Features TF-IDF** : 5000\n",
                "- **Pipeline** : Nettoyage ‚Üí Tokenization ‚Üí POS ‚Üí Lemmatization ‚Üí TF-IDF\n",
                "\n",
                "---\n",
                "\n",
                "**Note** : Le pipeline complet garantit que TF-IDF est calcul√© sur des textes enti√®rement pr√©trait√©s (lemmatis√©s), ce qui am√©liore la qualit√© des repr√©sentations vectorielles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-494061972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ajouter au d√©but du notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Puis copier les fichiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Ajouter au d√©but du notebook\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Puis copier les fichiers\n",
                "!cp -r preprocessed_data /content/drive/MyDrive/"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
