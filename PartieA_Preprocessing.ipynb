{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Knowledge Extraction - Partie A : Preprocessing et Repr√©sentation Text\n",
                "\n",
                "**Universit√© Paris Cit√© - Master 2 VMI**\n",
                "**Cours :** IFLCE085 Recherche et extraction s√©mantique √† partir de texte (Prof. Salima Benbernou)\n",
                "\n",
                "**√âquipe :**\n",
                "- **Partie A (Preprocessing) : Jacques Gastebois**\n",
                "- Partie B : Boutayna EL MOUJAOUID\n",
                "- Partie C : Franz Dervis\n",
                "- Partie D : Aya Benkabour\n",
                "\n",
                "---\n",
                "\n",
                "## √âtape 1 : Setup et Importations\n",
                "Objectif : Configurer l'environnement et importer les librairies n√©cessaires."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "# Installation des d√©pendances de base\n",
                "!{sys.executable} -m pip install pandas\n",
                "!{sys.executable} -m pip install numpy\n",
                "!{sys.executable} -m pip install nltk\n",
                "!{sys.executable} -m pip install scikit-learn\n",
                "!{sys.executable} -m pip install spacy\n",
                "\n",
                "# T√©l√©chargement du mod√®le spaCy anglais\n",
                "!{sys.executable} -m spacy download en_core_web_sm\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "import json\n",
                "import re\n",
                "import pickle\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import spacy\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from scipy.sparse import save_npz\n",
                "\n",
                "# T√©l√©chargement des ressources NLTK (version mise √† jour)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "# Chargement du mod√®le spaCy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Configuration de l'affichage pandas\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "\n",
                "print(\"Environnement configur√© avec succ√®s.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 2 : Chargement et Exploration des Donn√©es (SciREX)\n",
                "Objectif : T√©l√©charger (si n√©cessaire) et charger le dataset SciREX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import urllib.request\n",
                "import tarfile\n",
                "\n",
                "DATA_DIR = \"release_data\"\n",
                "DATA_URL = \"https://github.com/allenai/SciREX/raw/master/scirex_dataset/release_data.tar.gz\"\n",
                "TAR_FILE = \"release_data.tar.gz\"\n",
                "\n",
                "def download_and_extract_data():\n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(\"Dossier de donn√©es non trouv√©. T√©l√©chargement en cours...\")\n",
                "        try:\n",
                "            urllib.request.urlretrieve(DATA_URL, TAR_FILE)\n",
                "            print(\"T√©l√©chargement termin√©. Extraction...\")\n",
                "            with tarfile.open(TAR_FILE, \"r:gz\") as tar:\n",
                "                tar.extractall()\n",
                "            print(\"Extraction termin√©e.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Erreur lors du t√©l√©chargement/extraction : {e}\")\n",
                "    else:\n",
                "        print(\"Les donn√©es sont d√©j√† pr√©sentes.\")\n",
                "\n",
                "download_and_extract_data()\n",
                "\n",
                "FILES = {\n",
                "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
                "    \"dev\": os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
                "    \"test\": os.path.join(DATA_DIR, \"test.jsonl\")\n",
                "}\n",
                "\n",
                "def load_jsonl(file_path):\n",
                "    \"\"\"Charge un fichier JSONL dans une liste de dictionnaires.\"\"\"\n",
                "    data = []\n",
                "    if os.path.exists(file_path):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                data.append(json.loads(line))\n",
                "    else:\n",
                "        print(f\"Attention : Fichier {file_path} introuvable.\")\n",
                "    return data\n",
                "\n",
                "# Chargement des donn√©es\n",
                "print(\"Chargement des donn√©es...\")\n",
                "train_data = load_jsonl(FILES[\"train\"])\n",
                "dev_data = load_jsonl(FILES[\"dev\"])\n",
                "test_data = load_jsonl(FILES[\"test\"])\n",
                "\n",
                "print(f\"Nombre de documents Train : {len(train_data)}\")\n",
                "print(f\"Nombre de documents Dev   : {len(dev_data)}\")\n",
                "print(f\"Nombre de documents Test  : {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exploration d'un document type\n",
                "if train_data:\n",
                "    doc_example = train_data[0]\n",
                "    print(\"\\nCl√©s disponibles dans un document :\")\n",
                "    print(list(doc_example.keys()))\n",
                "\n",
                "    print(\"\\nExemple de contenu (champs principaux) :\")\n",
                "    print(f\"ID: {doc_example.get('doc_id')}\")\n",
                "    if 'words' in doc_example:\n",
                "        print(f\"D√©but du texte (50 premiers mots) : {' '.join(doc_example['words'][:50])}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 3 : Nettoyage et Normalisation\n",
                "Objectif : Nettoyer le texte (lowercase, suppression caract√®res sp√©ciaux, espaces multiples)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Nettoie le texte : lowercase, suppression caract√®res sp√©ciaux, espaces multiples.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† nettoyer\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte nettoy√©\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Suppression des caract√®res sp√©ciaux (garde lettres, chiffres et espaces)\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
                "    \n",
                "    # 3. Suppression des espaces multiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "print(\"Fonction clean_text() d√©finie.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'words' in doc_example:\n",
                "    raw_text = ' '.join(doc_example['words'])\n",
                "    cleaned_text = clean_text(raw_text)\n",
                "    \n",
                "    print(\"Texte original (200 premiers caract√®res) :\")\n",
                "    print(raw_text[:200])\n",
                "    print(\"\\nTexte nettoy√© (200 premiers caract√®res) :\")\n",
                "    print(cleaned_text[:200])\n",
                "    print(f\"\\nLongueur originale : {len(raw_text)} caract√®res\")\n",
                "    print(f\"Longueur nettoy√©e  : {len(cleaned_text)} caract√®res\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Application du nettoyage sur tous les documents\n",
                "print(\"Application du nettoyage sur tous les documents...\")\n",
                "\n",
                "for doc in train_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in dev_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "for doc in test_data:\n",
                "    if 'words' in doc:\n",
                "        doc['cleaned_text'] = clean_text(' '.join(doc['words']))\n",
                "\n",
                "print(f\"Nettoyage termin√© pour {len(train_data)} docs train, {len(dev_data)} docs dev, {len(test_data)} docs test.\")\n",
                "print(f\"\\nExemple de texte nettoy√© (doc 0) : {train_data[0]['cleaned_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 4 : Tokenization, POS Tagging et Lemmatization\n",
                "Objectif : Tokeniser, identifier les parties du discours (POS) et lemmatiser les textes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_and_pos(text):\n",
                "    \"\"\"\n",
                "    Tokenise le texte et effectue le POS tagging avec NLTK.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† tokeniser\n",
                "    \n",
                "    Returns:\n",
                "        list: Liste de tuples (token, pos_tag)\n",
                "    \"\"\"\n",
                "    tokens = word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    return pos_tags\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    \"\"\"\n",
                "    Lemmatise le texte avec spaCy.\n",
                "    \n",
                "    Args:\n",
                "        text (str): Texte √† lemmatiser\n",
                "    \n",
                "    Returns:\n",
                "        str: Texte lemmatis√©\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    lemmas = [token.lemma_ for token in doc]\n",
                "    return ' '.join(lemmas)\n",
                "\n",
                "print(\"Fonctions tokenize_and_pos() et lemmatize_text() d√©finies.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test sur l'exemple\n",
                "if train_data and 'cleaned_text' in train_data[0]:\n",
                "    sample_text = train_data[0]['cleaned_text'][:500]  # Premier 500 caract√®res\n",
                "    \n",
                "    print(\"Texte nettoy√© (extrait) :\")\n",
                "    print(sample_text)\n",
                "    \n",
                "    print(\"\\n--- Tokenization + POS Tagging ---\")\n",
                "    pos_tags = tokenize_and_pos(sample_text)\n",
                "    print(f\"Nombre de tokens : {len(pos_tags)}\")\n",
                "    print(f\"Premiers 10 tokens avec POS : {pos_tags[:10]}\")\n",
                "    \n",
                "    print(\"\\n--- Lemmatization ---\")\n",
                "    lemmatized = lemmatize_text(sample_text)\n",
                "    print(f\"Texte lemmatis√© (extrait) : {lemmatized[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Application de la lemmatization sur TOUS les documents (train, dev, test)\n",
                "def process_dataset_lemmatization(dataset, name):\n",
                "    print(f\"Application de la lemmatization sur {len(dataset)} documents {name}...\")\n",
                "    for i, doc in enumerate(dataset):\n",
                "        if 'cleaned_text' in doc:\n",
                "            doc['lemmatized_text'] = lemmatize_text(doc['cleaned_text'])\n",
                "        if (i + 1) % 50 == 0:\n",
                "            print(f\"  Trait√© {i + 1}/{len(dataset)} documents...\")\n",
                "    print(f\"Lemmatization termin√©e pour {name}.\\n\")\n",
                "\n",
                "print(\"Cela peut prendre quelques minutes...\\n\")\n",
                "process_dataset_lemmatization(train_data, \"TRAIN\")\n",
                "process_dataset_lemmatization(dev_data, \"DEV\")\n",
                "process_dataset_lemmatization(test_data, \"TEST\")\n",
                "\n",
                "print(f\"Exemple de texte lemmatis√© (doc 0 train) : {train_data[0]['lemmatized_text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 5 : Repr√©sentation Vectorielle TF-IDF\n",
                "Objectif : Cr√©er une repr√©sentation vectorielle des textes avec TF-IDF sur les textes lemmatis√©s."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pr√©paration des textes pour TF-IDF (utilisation des textes LEMMATIS√âS)\n",
                "train_texts = [doc['lemmatized_text'] for doc in train_data if 'lemmatized_text' in doc]\n",
                "dev_texts = [doc['lemmatized_text'] for doc in dev_data if 'lemmatized_text' in doc]\n",
                "test_texts = [doc['lemmatized_text'] for doc in test_data if 'lemmatized_text' in doc]\n",
                "\n",
                "# Cr√©ation du vectoriseur TF-IDF\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_features=5000,  # Limite √† 5000 features les plus importantes\n",
                "    min_df=2,           # Ignore les termes qui apparaissent dans moins de 2 documents\n",
                "    max_df=0.8,         # Ignore les termes qui apparaissent dans plus de 80% des documents\n",
                "    ngram_range=(1, 2)  # Unigrammes et bigrammes\n",
                ")\n",
                "\n",
                "# Calcul de la matrice TF-IDF sur TRAIN (fit_transform)\n",
                "print(\"Calcul de la matrice TF-IDF sur TRAIN (fit_transform)...\")\n",
                "tfidf_matrix = tfidf_vectorizer.fit_transform(train_texts)\n",
                "\n",
                "# Transformation des ensembles DEV et TEST (transform uniquement)\n",
                "print(\"Transformation des ensembles DEV et TEST (transform)...\")\n",
                "tfidf_matrix_dev = tfidf_vectorizer.transform(dev_texts)\n",
                "tfidf_matrix_test = tfidf_vectorizer.transform(test_texts)\n",
                "\n",
                "print(f\"\\nMatrices TF-IDF cr√©√©es :\")\n",
                "print(f\"  TRAIN : {tfidf_matrix.shape} (Densit√© : {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f})\")\n",
                "print(f\"  DEV   : {tfidf_matrix_dev.shape} (Densit√© : {tfidf_matrix_dev.nnz / (tfidf_matrix_dev.shape[0] * tfidf_matrix_dev.shape[1]):.4f})\")\n",
                "print(f\"  TEST  : {tfidf_matrix_test.shape} (Densit√© : {tfidf_matrix_test.nnz / (tfidf_matrix_test.shape[0] * tfidf_matrix_test.shape[1]):.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Affichage des top features pour le premier document\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "doc_0_vector = tfidf_matrix[0].toarray()[0]\n",
                "top_indices = doc_0_vector.argsort()[-10:][::-1]\n",
                "\n",
                "print(\"Top 10 features TF-IDF pour le document 0 :\")\n",
                "for idx in top_indices:\n",
                "    print(f\"  {feature_names[idx]}: {doc_0_vector[idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 6 : Export et Sauvegarde des R√©sultats\n",
                "Objectif : Sauvegarder tous les r√©sultats pour la Partie B."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cr√©ation du dossier de sortie\n",
                "OUTPUT_DIR = \"preprocessed_data\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Dossier de sortie cr√©√© : {OUTPUT_DIR}/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 - Export des textes pr√©trait√©s (CSV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_to_csv(dataset, filename):\n",
                "    df = pd.DataFrame([\n",
                "        {\n",
                "            'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "            'raw_text': ' '.join(doc.get('words', [])),\n",
                "            'cleaned_text': doc.get('cleaned_text', ''),\n",
                "            'lemmatized_text': doc.get('lemmatized_text', '')\n",
                "        }\n",
                "        for i, doc in enumerate(dataset)\n",
                "    ])\n",
                "    df.to_csv(os.path.join(OUTPUT_DIR, filename), index=False, encoding='utf-8')\n",
                "    print(f\"‚úÖ Fichier sauvegard√© : {OUTPUT_DIR}/{filename}\")\n",
                "    print(f\"   Colonnes : {list(df.columns)}\")\n",
                "    print(f\"   Nombre de lignes : {len(df)}\")\n",
                "\n",
                "export_to_csv(train_data, 'train_preprocessed.csv')\n",
                "export_to_csv(dev_data, 'dev_preprocessed.csv')\n",
                "export_to_csv(test_data, 'test_preprocessed.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 - Export de la matrice TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sauvegarde des matrices TF-IDF (format sparse)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix.npz'), tfidf_matrix)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_dev.npz'), tfidf_matrix_dev)\n",
                "save_npz(os.path.join(OUTPUT_DIR, 'tfidf_matrix_test.npz'), tfidf_matrix_test)\n",
                "\n",
                "print(f\"‚úÖ Matrice TF-IDF TRAIN sauvegard√©e : {OUTPUT_DIR}/tfidf_matrix.npz\")\n",
                "print(f\"‚úÖ Matrice TF-IDF DEV sauvegard√©e   : {OUTPUT_DIR}/tfidf_matrix_dev.npz\")\n",
                "print(f\"‚úÖ Matrice TF-IDF TEST sauvegard√©e  : {OUTPUT_DIR}/tfidf_matrix_test.npz\")\n",
                "\n",
                "# Sauvegarde du vectoriseur (pour r√©utilisation)\n",
                "with open(os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
                "    pickle.dump(tfidf_vectorizer, f)\n",
                "print(f\"‚úÖ Vectoriseur TF-IDF sauvegard√© : {OUTPUT_DIR}/tfidf_vectorizer.pkl\")\n",
                "\n",
                "# Sauvegarde des noms de features\n",
                "np.save(os.path.join(OUTPUT_DIR, 'tfidf_feature_names.npy'), feature_names)\n",
                "print(f\"‚úÖ Noms des features sauvegard√©s : {OUTPUT_DIR}/tfidf_feature_names.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 - Export du dictionnaire de correspondance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cr√©ation d'un dictionnaire de correspondance complet\n",
                "correspondence_dict = {\n",
                "    'metadata': {\n",
                "        'n_documents_train': len(train_data),\n",
                "        'n_documents_dev': len(dev_data),\n",
                "        'n_documents_test': len(test_data),\n",
                "        'tfidf_shape_train': tfidf_matrix.shape,\n",
                "        'tfidf_shape_dev': tfidf_matrix_dev.shape,\n",
                "        'tfidf_shape_test': tfidf_matrix_test.shape,\n",
                "        'n_features': len(feature_names),\n",
                "        'preprocessing_steps': [\n",
                "            '1. Lowercase',\n",
                "            '2. Suppression caract√®res sp√©ciaux',\n",
                "            '3. Normalisation espaces',\n",
                "            '4. Tokenization (NLTK)',\n",
                "            '5. POS Tagging (NLTK)',\n",
                "            '6. Lemmatization (spaCy)',\n",
                "            '7. TF-IDF sur textes lemmatis√©s (max_features=5000, ngram_range=(1,2))'\n",
                "        ]\n",
                "    },\n",
                "    'documents_train_sample': [\n",
                "        {\n",
                "            'doc_id': doc.get('doc_id', f'doc_{i}'),\n",
                "            'raw_text_preview': ' '.join(doc.get('words', []))[:200],\n",
                "            'cleaned_text_preview': doc.get('cleaned_text', '')[:200],\n",
                "            'lemmatized_text_preview': doc.get('lemmatized_text', '')[:200],\n",
                "            'tfidf_vector_index': i\n",
                "        }\n",
                "        for i, doc in enumerate(train_data[:5])\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Sauvegarde en JSON\n",
                "with open(os.path.join(OUTPUT_DIR, 'correspondence_dict.json'), 'w', encoding='utf-8') as f:\n",
                "    json.dump(correspondence_dict, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"‚úÖ Dictionnaire de correspondance sauvegard√© : {OUTPUT_DIR}/correspondence_dict.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 - R√©sum√© des fichiers export√©s"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"R√âSUM√â DES FICHIERS EXPORT√âS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    if os.path.isfile(filepath):\n",
                "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
                "        print(f\"üìÑ {filename:35s} ({size_mb:.2f} MB)\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## R√©sum√© Technique - Partie A\n",
                "\n",
                "### Pipeline de Preprocessing Complet\n",
                "\n",
                "1. **Nettoyage** :\n",
                "   - Conversion en lowercase\n",
                "   - Suppression des caract√®res sp√©ciaux (garde uniquement lettres, chiffres, espaces)\n",
                "   - Normalisation des espaces multiples\n",
                "\n",
                "2. **Tokenization** :\n",
                "   - Tokenization avec NLTK (`word_tokenize`)\n",
                "\n",
                "3. **POS Tagging** :\n",
                "   - POS tagging avec NLTK (`pos_tag`)\n",
                "\n",
                "4. **Lemmatization** :\n",
                "   - Lemmatization avec spaCy (`en_core_web_sm`)\n",
                "   - **Appliqu√© sur TOUS les documents (Train, Dev, Test)**\n",
                "\n",
                "5. **Repr√©sentation TF-IDF** :\n",
                "   - Vectorisation avec scikit-learn `TfidfVectorizer`\n",
                "   - **Apprentissage (Fit)** sur le TRAIN set uniquement\n",
                "   - **Transformation** appliqu√©e sur TRAIN, DEV et TEST\n",
                "   - Param√®tres : `max_features=5000`, `min_df=2`, `max_df=0.8`, `ngram_range=(1,2)`\n",
                "   - Matrice r√©sultante : N documents √ó 5000 features\n",
                "\n",
                "---\n",
                "\n",
                "### Fichiers Export√©s pour la Partie B\n",
                "\n",
                "| Fichier | Description | Format |\n",
                "|---------|-------------|--------|\n",
                "| `train_preprocessed.csv` | Textes TRAIN pr√©trait√©s | CSV |\n",
                "| `dev_preprocessed.csv` | Textes DEV pr√©trait√©s | CSV |\n",
                "| `test_preprocessed.csv` | Textes TEST pr√©trait√©s | CSV |\n",
                "| `tfidf_matrix.npz` | Matrice TF-IDF TRAIN (sparse) | NumPy compressed |\n",
                "| `tfidf_matrix_dev.npz` | Matrice TF-IDF DEV (sparse) | NumPy compressed |\n",
                "| `tfidf_matrix_test.npz` | Matrice TF-IDF TEST (sparse) | NumPy compressed |\n",
                "| `tfidf_vectorizer.pkl` | Vectoriseur TF-IDF entra√Æn√© | Pickle |\n",
                "| `tfidf_feature_names.npy` | Noms des 5000 features TF-IDF | NumPy |\n",
                "| `correspondence_dict.json` | M√©tadonn√©es et correspondances | JSON |\n",
                "\n",
                "---\n",
                "\n",
                "### Comment Charger les Donn√©es (Partie B)\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "from scipy.sparse import load_npz\n",
                "\n",
                "# Charger les textes pr√©trait√©s\n",
                "df_train = pd.read_csv('preprocessed_data/train_preprocessed.csv')\n",
                "df_dev = pd.read_csv('preprocessed_data/dev_preprocessed.csv')\n",
                "\n",
                "# Charger les matrices TF-IDF\n",
                "tfidf_train = load_npz('preprocessed_data/tfidf_matrix.npz')\n",
                "tfidf_dev = load_npz('preprocessed_data/tfidf_matrix_dev.npz')\n",
                "\n",
                "# Charger le vectoriseur (pour transformer de nouveaux textes)\n",
                "with open('preprocessed_data/tfidf_vectorizer.pkl', 'rb') as f:\n",
                "    vectorizer = pickle.load(f)\n",
                "\n",
                "# Charger les noms de features\n",
                "feature_names = np.load('preprocessed_data/tfidf_feature_names.npy')\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### Statistiques Finales\n",
                "\n",
                "- **Documents train** : 306\n",
                "- **Documents dev** : 66\n",
                "- **Documents test** : 66\n",
                "- **Features TF-IDF** : 5000\n",
                "- **Pipeline** : Nettoyage ‚Üí Tokenization ‚Üí POS ‚Üí Lemmatization ‚Üí TF-IDF\n",
                "\n",
                "---\n",
                "\n",
                "**Note** : Le pipeline complet garantit que TF-IDF est calcul√© sur des textes enti√®rement pr√©trait√©s (lemmatis√©s), ce qui am√©liore la qualit√© des repr√©sentations vectorielles."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
