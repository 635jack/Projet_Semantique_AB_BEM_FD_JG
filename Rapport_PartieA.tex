\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}

\geometry{margin=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Rapport Partie A - Preprocessing},
    pdfauthor={Jacques Gastebois},
}

% Configuration du code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
}

% En-tête et pied de page
\pagestyle{fancy}
\fancyhf{}
\rhead{Partie A : Preprocessing}
\lhead{Projet Sémantique}
\cfoot{\thepage}

\title{\textbf{Projet Knowledge Extraction from Unstructured Text}\\
\large Partie A : Preprocessing et Représentation Textuelle\\
\vspace{0.5cm}
\normalsize Rapport Technique}

\author{
Jacques Gastebois\\
\small Master 2 VMI - Université Paris Cité\\
\small IFLCE085 - Recherche et extraction sémantique à partir de texte\\
\small Prof. Salima Benbernou
}

\date{30 Novembre 2025}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport présente la méthodologie complète de preprocessing appliquée au dataset SciREX dans le cadre du projet d'extraction de connaissances à partir de textes non structurés. Le pipeline implémenté comprend le nettoyage, la tokenization, le POS tagging, la lemmatization et la vectorisation TF-IDF. Les résultats sont exportés dans des formats exploitables pour la Partie B du projet.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Contexte du Projet}
Ce projet s'inscrit dans le cadre du cours IFLCE085 et vise à développer un système complet d'extraction de connaissances à partir de textes scientifiques non structurés. La Partie A se concentre sur le preprocessing et la préparation des données.

\subsection{Objectifs}
\begin{itemize}
    \item Nettoyer et normaliser le dataset SciREX (306 documents train)
    \item Appliquer un pipeline NLP complet (tokenization, POS tagging, lemmatization)
    \item Générer des représentations vectorielles TF-IDF
    \item Exporter les résultats dans des formats réutilisables pour la Partie B
\end{itemize}

\subsection{Dataset}
\textbf{SciREX} (Scientific Information Extraction) : Dataset de 438 articles scientifiques annotés provenant de AI2.
\begin{itemize}
    \item \textbf{Train} : 306 documents
    \item \textbf{Dev} : 66 documents
    \item \textbf{Test} : 66 documents
\end{itemize}

\section{Méthodologie}

\subsection{Pipeline de Preprocessing}

Le pipeline complet suit une approche séquentielle en 5 étapes :

\begin{enumerate}[label=\textbf{Étape \arabic*:}]
    \item \textbf{Nettoyage et Normalisation}
    \begin{itemize}
        \item Conversion en lowercase
        \item Suppression des caractères spéciaux (garde lettres, chiffres, espaces)
        \item Normalisation des espaces multiples
    \end{itemize}
    
    \item \textbf{Tokenization}
    \begin{itemize}
        \item Outil : NLTK \texttt{word\_tokenize}
        \item Découpage du texte en tokens individuels
    \end{itemize}
    
    \item \textbf{POS Tagging}
    \begin{itemize}
        \item Outil : NLTK \texttt{pos\_tag}
        \item Annotation grammaticale (nom, verbe, adjectif, etc.)
    \end{itemize}
    
    \item \textbf{Lemmatization}
    \begin{itemize}
        \item Outil : spaCy \texttt{en\_core\_web\_sm}
        \item Réduction des mots à leur forme canonique
        \item \textbf{Appliqué sur TOUS les 306 documents train}
    \end{itemize}
    
    \item \textbf{Vectorisation TF-IDF}
    \begin{itemize}
        \item Outil : scikit-learn \texttt{TfidfVectorizer}
        \item \textbf{Calculé sur les textes lemmatisés} (pipeline complet)
        \item Paramètres : \texttt{max\_features=5000}, \texttt{ngram\_range=(1,2)}
    \end{itemize}
\end{enumerate}

\subsection{Justification des Choix Techniques}

\subsubsection{Nettoyage}
Le nettoyage est essentiel pour réduire le bruit et standardiser le texte. La suppression des caractères spéciaux permet de se concentrer sur le contenu sémantique.

\subsubsection{Lemmatization vs Stemming}
La lemmatization a été préférée au stemming car elle produit des formes linguistiquement correctes (ex: "better" → "good" au lieu de "bett").

\subsubsection{TF-IDF sur Textes Lemmatisés}
\textbf{Décision critique} : Le TF-IDF est calculé sur les textes lemmatisés (et non sur les textes nettoyés) pour garantir :
\begin{itemize}
    \item Une réduction du vocabulaire (formes canoniques)
    \item Une meilleure capture des concepts sémantiques
    \item Une cohérence avec le pipeline NLP complet
\end{itemize}

\section{Implémentation}

\subsection{Technologies Utilisées}

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Librairie} & \textbf{Version/Utilisation} \\ \midrule
Python & 3.12 \\
pandas & 2.2.2 (manipulation de données) \\
numpy & 2.0.2 (calculs numériques) \\
NLTK & 3.9.1 (tokenization, POS tagging) \\
spaCy & 3.8.11 (lemmatization) \\
scikit-learn & 1.6.1 (TF-IDF) \\
scipy & 1.16.3 (matrices sparse) \\ \bottomrule
\end{tabular}
\caption{Stack technologique}
\end{table}

\subsection{Exemple de Code}

\subsubsection{Fonction de Nettoyage}
\begin{lstlisting}[language=Python]
def clean_text(text):
    """Nettoie le texte."""
    if not isinstance(text, str):
        return ""
    
    # 1. Lowercase
    text = text.lower()
    
    # 2. Suppression caracteres speciaux
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    
    # 3. Normalisation espaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
\end{lstlisting}

\subsubsection{Lemmatization avec spaCy}
\begin{lstlisting}[language=Python]
def lemmatize_text(text):
    """Lemmatise le texte avec spaCy."""
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc]
    return ' '.join(lemmas)
\end{lstlisting}

\subsubsection{Vectorisation TF-IDF}
\begin{lstlisting}[language=Python]
# Preparation des textes LEMMATISES
train_texts = [doc['lemmatized_text'] 
               for doc in train_data 
               if 'lemmatized_text' in doc]

# Vectorisation
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    min_df=2,
    max_df=0.8,
    ngram_range=(1, 2)
)

tfidf_matrix = tfidf_vectorizer.fit_transform(train_texts)
\end{lstlisting}

\section{Résultats}

\subsection{Statistiques du Preprocessing}

\begin{table}[h]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Métrique} & \textbf{Valeur} \\ \midrule
Documents train traités & 306 \\
Documents dev traités & 66 \\
Documents test traités & 66 \\
Documents lemmatisés & 306 (100\%) \\
Taille matrice TF-IDF & 306 × 5000 \\
Densité matrice & 23.7\% \\
Vocabulaire TF-IDF & 5000 features \\ \bottomrule
\end{tabular}
\caption{Statistiques de preprocessing}
\end{table}

\subsection{Exemple de Transformation}

\textbf{Texte original :}
\begin{quote}
\textit{"Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes"}
\end{quote}

\textbf{Texte nettoyé :}
\begin{quote}
\textit{"full resolution residual networks for semantic segmentation in street scenes"}
\end{quote}

\textbf{Texte lemmatisé :}
\begin{quote}
\textit{"full resolution residual network for semantic segmentation in street scene"}
\end{quote}

\subsection{Top 10 Features TF-IDF (Document 0)}

\begin{table}[h]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Feature} & \textbf{Score TF-IDF} \\ \midrule
stream & 0.2819 \\
cityscapes & 0.2445 \\
residual & 0.2226 \\
resolution & 0.1905 \\
segmentation & 0.1893 \\
reference reference & 0.1770 \\
pooling & 0.1688 \\
pooling operations & 0.1449 \\
image & 0.1281 \\
semantic segmentation & 0.1195 \\ \bottomrule
\end{tabular}
\caption{Top 10 features TF-IDF pour le premier document}
\end{table}

\section{Export des Données}

\subsection{Fichiers Générés}

Tous les fichiers sont exportés dans le dossier \texttt{preprocessed\_data/} :

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Fichier} & \textbf{Format} & \textbf{Taille} \\ \midrule
\texttt{train\_preprocessed.csv} & CSV & 20.26 MB \\
\texttt{tfidf\_matrix.npz} & NumPy sparse & 1.96 MB \\
\texttt{tfidf\_vectorizer.pkl} & Pickle & 0.19 MB \\
\texttt{tfidf\_feature\_names.npy} & NumPy & 0.06 MB \\
\texttt{correspondence\_dict.json} & JSON & 0.01 MB \\ \bottomrule
\end{tabular}
\caption{Fichiers exportés}
\end{table}

\subsection{Structure du CSV}

Le fichier \texttt{train\_preprocessed.csv} contient 4 colonnes :
\begin{itemize}
    \item \texttt{doc\_id} : Identifiant unique du document
    \item \texttt{raw\_text} : Texte original
    \item \texttt{cleaned\_text} : Texte nettoyé
    \item \texttt{lemmatized\_text} : Texte lemmatisé
\end{itemize}

\section{Utilisation pour la Partie B}

\subsection{Chargement des Données}

\subsubsection{Textes Prétraités}
\begin{lstlisting}[language=Python]
import pandas as pd

# Charger le CSV
df = pd.read_csv('preprocessed_data/train_preprocessed.csv')

# Acceder aux differentes versions
raw_texts = df['raw_text']
cleaned_texts = df['cleaned_text']
lemmatized_texts = df['lemmatized_text']
\end{lstlisting}

\subsubsection{Matrice TF-IDF}
\begin{lstlisting}[language=Python]
from scipy.sparse import load_npz
import pickle
import numpy as np

# Charger la matrice TF-IDF
tfidf_matrix = load_npz('preprocessed_data/tfidf_matrix.npz')

# Charger le vectoriseur (pour transformer de nouveaux textes)
with open('preprocessed_data/tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

# Charger les noms de features
feature_names = np.load('preprocessed_data/tfidf_feature_names.npy')

# Transformer un nouveau texte
new_text = ["your new text here"]
new_vector = vectorizer.transform(new_text)
\end{lstlisting}

\subsubsection{Métadonnées}
\begin{lstlisting}[language=Python]
import json

# Charger le dictionnaire de correspondance
with open('preprocessed_data/correspondence_dict.json', 'r') as f:
    metadata = json.load(f)

# Acceder aux informations
n_docs = metadata['metadata']['n_documents']
preprocessing_steps = metadata['metadata']['preprocessing_steps']
\end{lstlisting}

\subsection{Recommandations pour la Partie B}

\begin{enumerate}
    \item \textbf{Utiliser les textes lemmatisés} pour toute analyse sémantique
    \item \textbf{Réutiliser le vectoriseur TF-IDF} pour transformer de nouveaux documents
    \item \textbf{Exploiter la matrice sparse} pour économiser la mémoire
    \item \textbf{Consulter le dictionnaire de correspondance} pour comprendre le pipeline
\end{enumerate}

\section{Accès aux Ressources}

\subsection{Liens Importants}

\begin{itemize}
    \item \textbf{GitHub Repository :} \\
    \url{https://github.com/635jack/Projet_Semantique_AB_BEM_FD_JG}
    
    \item \textbf{Google Colab Notebook :} \\
    \url{https://colab.research.google.com/github/635jack/Projet_Semantique_AB_BEM_FD_JG/blob/master/PartieA_Preprocessing.ipynb}
    
    \item \textbf{Dataset SciREX :} \\
    \url{https://github.com/allenai/SciREX}
\end{itemize}

\subsection{Instructions d'Exécution}

\subsubsection{Sur Google Colab}
\begin{enumerate}
    \item Ouvrir le lien Colab ci-dessus
    \item Exécuter toutes les cellules : \texttt{Runtime → Run all}
    \item Télécharger les fichiers exportés depuis \texttt{preprocessed\_data/}
\end{enumerate}

\subsubsection{En Local}
\begin{enumerate}
    \item Cloner le repository : \\
    \texttt{git clone https://github.com/635jack/Projet\_Semantique\_AB\_BEM\_FD\_JG}
    \item Installer les dépendances : \\
    \texttt{pip install -r requirements.txt}
    \item Exécuter le notebook : \\
    \texttt{jupyter notebook PartieA\_Preprocessing.ipynb}
\end{enumerate}

\section{Conclusion}

\subsection{Résumé des Contributions}

Ce travail a permis de :
\begin{itemize}
    \item Implémenter un pipeline NLP complet et robuste
    \item Traiter l'intégralité du dataset SciREX (306 documents)
    \item Générer des représentations vectorielles de haute qualité
    \item Exporter les résultats dans des formats standardisés
\end{itemize}

\subsection{Points Clés}

\begin{enumerate}
    \item \textbf{Pipeline complet} : Nettoyage → Tokenization → POS → Lemmatization → TF-IDF
    \item \textbf{TF-IDF sur textes lemmatisés} : Garantit la cohérence du preprocessing
    \item \textbf{Tous les documents traités} : 306/306 documents lemmatisés
    \item \textbf{Formats réutilisables} : CSV, NPZ, Pickle, JSON
\end{enumerate}

\subsection{Perspectives pour la Partie B}

Les données préparées sont prêtes pour :
\begin{itemize}
    \item Extraction d'entités nommées (NER)
    \item Classification de documents
    \item Clustering sémantique
    \item Analyse de relations
    \item Modèles d'embeddings avancés
\end{itemize}

\section*{Annexes}

\subsection*{A. Commandes Utiles}

\textbf{Vérifier la taille des fichiers :}
\begin{lstlisting}[language=bash]
ls -lh preprocessed_data/
\end{lstlisting}

\textbf{Compter les lignes du CSV :}
\begin{lstlisting}[language=bash]
wc -l preprocessed_data/train_preprocessed.csv
\end{lstlisting}

\subsection*{B. Dépendances Complètes}

Voir \texttt{requirements.txt} dans le repository GitHub.

\end{document}
