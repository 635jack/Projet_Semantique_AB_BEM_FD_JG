\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}

\geometry{margin=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Rapport Final Partie A - Preprocessing},
    pdfauthor={Jacques Gastebois},
}

% Configuration du code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    language=Python
}

% En-tête et pied de page
\pagestyle{fancy}
\fancyhf{}
\rhead{Partie A : Preprocessing \& Analyse}
\lhead{Projet Sémantique}
\cfoot{\thepage}

\title{\textbf{Projet Knowledge Extraction}\\
\large Partie A : Preprocessing, Analyse Statistique et Justification des Choix\\
\vspace{0.5cm}
\normalsize Rapport Technique Final}

\author{
Jacques Gastebois\\
\small Master 2 VMI - Université Paris Cité\\
\small IFLCE085 - Recherche et extraction sémantique
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport synthétise les travaux réalisés pour la Partie A du projet. Il détaille le pipeline de preprocessing appliqué au corpus NER (2221 phrases), présente une analyse statistique des données, et justifie les choix techniques (nettoyage, lemmatization) en analysant leurs avantages, inconvénients et impacts sur les tâches d'extraction d'entités (Partie B) et de relations (Partie C).
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
Le projet vise à extraire des connaissances structurées à partir d'un corpus de textes non structurés. La première étape cruciale est le \textbf{preprocessing}, qui transforme les données brutes en un format exploitable pour les algorithmes d'apprentissage automatique.

\subsection{Le Corpus}
Le dataset utilisé (\texttt{data.csv}) est constitué de \textbf{2221 phrases} annotées pour la reconnaissance d'entités nommées (NER).
\begin{itemize}
    \item \textbf{Format d'entrée} : CSV avec colonnes \texttt{id}, \texttt{words}, \texttt{ner\_tags}, \texttt{text}.
    \item \textbf{Contenu} : Textes encyclopédiques/biographiques.
    \item \textbf{Annotations} : Tags BIO (Begin, Inside, Outside) pour les entités Personnes (PER), Lieux (LOC), Organisations (ORG), etc.
\end{itemize}

\section{Méthodologie de Preprocessing}

Contrairement à une approche classique de "Bag of Words" (TF-IDF), nous avons opté pour une approche de \textbf{preprocessing enrichi} qui préserve la structure séquentielle des données, essentielle pour le NER.

\subsection{Pipeline Mis en Place}
Le traitement a été réalisé en Python (via un notebook Jupyter) et comprend les étapes suivantes :

\begin{enumerate}
    \item \textbf{Nettoyage (\texttt{cleaned\_text})} :
    \begin{itemize}
        \item Conversion en minuscules (lowercase) pour réduire la dimensionnalité.
        \item Suppression des caractères spéciaux non alphanumériques (sauf espaces).
        \item Normalisation des espaces multiples.
    \end{itemize}
    
    \item \textbf{Lemmatization (\texttt{lemmatized\_text})} :
    \begin{itemize}
        \item Utilisation de la librairie \textbf{spaCy} (modèle \texttt{en\_core\_web\_sm}).
        \item Transformation des mots en leur forme canonique (ex: "running" $\rightarrow$ "run", "better" $\rightarrow$ "good").
    \end{itemize}
    
    \item \textbf{Enrichissement du Dataset} :
    \begin{itemize}
        \item Au lieu de créer des matrices séparées, nous avons ajouté les colonnes \texttt{cleaned\_text} et \texttt{lemmatized\_text} directement au fichier original.
        \item \textbf{Sortie} : \texttt{data\_preprocessed.csv}.
    \end{itemize}
\end{enumerate}

\section{Analyse Statistique du Corpus}

L'analyse réalisée dans le notebook \texttt{PartieA\_Analyse\_Statistique.ipynb} a révélé les caractéristiques suivantes :

\subsection{Volumétrie et Longueur}
\begin{itemize}
    \item \textbf{Nombre de phrases} : 2221.
    \item \textbf{Longueur moyenne} : $\sim$24 mots par phrase.
    \item \textbf{Impact du preprocessing} : La lemmatization réduit la taille du vocabulaire d'environ \textbf{15\% à 25\%}, ce qui densifie l'information sans perte majeure de sens.
\end{itemize}

\subsection{Analyse POS (Part-of-Speech)}
Le corpus a été analysé avec spaCy pour identifier les catégories grammaticales. Sur 17 151 tokens analysés (échantillon de 700 phrases), la distribution révèle les patterns suivants :

\begin{table}[h]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{POS Tag} & \textbf{Count} & \textbf{Pourcentage} \\ \midrule
NOUN (Nom) & 2994 & 17.46\% \\
PROPN (Nom propre) & 2241 & 13.07\% \\
ADP (Préposition) & 2104 & 12.27\% \\
PUNCT (Ponctuation) & 2049 & 11.95\% \\
DET (Déterminant) & 1562 & 9.11\% \\ \bottomrule
\end{tabular}
\caption{Top 5 des catégories POS}
\end{table}

La figure \ref{fig:pos} montre la distribution complète des 15 principales catégories POS. On observe une forte présence de noms et noms propres (30.5\% combinés), typique des textes encyclopédiques/biographiques axés sur des personnages et des lieux.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{pos_distribution.png}
    \caption{Distribution des catégories POS (Top 15)}
    \label{fig:pos}
\end{figure}

\subsection{Vectorisation TF-IDF}
Le corpus lemmatisé a été transformé en représentation vectorielle TF-IDF, produisant une matrice de \textbf{700 documents × 2493 features}.

\textbf{Caractéristiques de la matrice :}
\begin{itemize}
    \item \textbf{Vocabulaire} : 2493 termes uniques (unigrammes et bigrammes)
    \item \textbf{Sparsité} : 0.77\% (matrice très sparse, typique de TF-IDF)
    \item \textbf{Paramètres} : max\_df=0.8, min\_df=2, ngram\_range=(1,2)
\end{itemize}

La figure \ref{fig:tfidf} présente les 20 termes avec les scores TF-IDF moyens les plus élevés. On note la prédominance d'articles et connecteurs (the, of, in), ce qui est attendu mais peut être filtré pour des analyses sémantiques plus fines.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{tfidf_top_terms.png}
    \caption{Top 20 termes par score TF-IDF moyen}
    \label{fig:tfidf}
\end{figure}

\textbf{Applications :}
\begin{itemize}
    \item Classification de documents
    \item Extraction de mots-clés
    \item Analyse de similarité entre phrases
    \item Clustering sémantique
\end{itemize}


\section{Analyse Critique des Choix}

\subsection{Choix 1 : Lemmatization vs Stemming}
\begin{itemize}
    \item \textbf{Choix} : Lemmatization (spaCy).
    \item \textbf{Avantages} : Produit des mots réels (formes dictionnaire), préserve mieux le sens sémantique pour l'extraction de relations (Partie C). Plus précis que le stemming (qui tronque brutalement).
    \item \textbf{Inconvénients} : Plus coûteux en temps de calcul. Peut perdre certaines nuances flexionnelles (temps des verbes) utiles pour la temporalité.
\end{itemize}

\subsection{Choix 2 : Lowercasing (Mise en minuscule)}
\begin{itemize}
    \item \textbf{Choix} : Tout mettre en minuscule dans \texttt{cleaned\_text}.
    \item \textbf{Avantages} : Réduit drastiquement la taille du vocabulaire. Associe "Apple" (fruit) et "apple" (fruit).
    \item \textbf{Inconvénients (Critique pour le NER)} : \textbf{Perte de l'information de capitalisation}, qui est un indice crucial pour détecter les entités nommées (ex: "Apple" l'entreprise vs "apple" le fruit).
    \item \textbf{Mitigation} : Nous avons conservé la colonne \texttt{text} originale. Les modèles NER modernes (BERT, etc.) ou les features manuelles peuvent utiliser le texte brut pour récupérer la casse.
\end{itemize}

\subsection{Choix 3 : Conservation de la Structure Séquentielle}
\begin{itemize}
    \item \textbf{Choix} : Ne pas vectoriser en TF-IDF (Bag of Words) pour l'export final.
    \item \textbf{Avantages} : Indispensable pour le NER et l'extraction de relations qui dépendent de l'ordre des mots et du contexte local. Permet l'utilisation de modèles de Deep Learning (RNN, Transformers).
    \item \textbf{Inconvénients} : Fichiers texte plus volumineux que des matrices creuses.
\end{itemize}

\section{Impact sur la Suite du Projet}

\subsection{Impact sur la Partie II (NER)}
Le preprocessing fournit une base propre (\texttt{lemmatized\_text}) qui généralise mieux. Cependant, pour maximiser les scores F1, il sera probablement nécessaire d'utiliser :
\begin{enumerate}
    \item Le \textbf{texte original} pour les features de capitalisation.
    \item Le \textbf{texte lemmatisé} pour les embeddings sémantiques (Word2Vec/GloVe) afin de regrouper les variantes d'un même mot.
\end{enumerate}

\subsection{Impact sur la Partie C (Extraction de Relations)}
L'extraction de relations (triplets Sujet-Verbe-Objet) bénéficie grandement de la lemmatization :
\begin{itemize}
    \item Elle normalise les verbes (ex: "est né", "naquit" $\rightarrow$ "naître"), simplifiant la détection de patterns de relations.
    \item Elle réduit la complexité des arbres de dépendance syntaxique.
\end{itemize}

\section{Conclusion}
Le preprocessing réalisé est un compromis entre \textbf{réduction de bruit} (nettoyage, lemmatization) et \textbf{préservation de l'information} (conservation du texte brut et de la séquence). Ce socle de données enrichi (\texttt{data\_preprocessed.csv}) est robuste et adapté aux défis des parties suivantes : la reconnaissance fine d'entités et l'extraction complexe de relations sémantiques.

\end{document}
