\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}

\geometry{top=1cm, bottom=2cm, left=2.5cm, right=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Rapport Final Partie A - Preprocessing},
    pdfauthor={Jacques Gastebois},
}

% Configuration du code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    language=Python
}

% En-tête et pied de page
\pagestyle{fancy}
\fancyhf{}
\rhead{Partie A : Preprocessing \& Analyse}
\lhead{Projet Sémantique}
\cfoot{\thepage}

\title{\textbf{Projet Knowledge Extraction - Partie A}}

\author{
Jacques Gastebois\\
\small Master 2 VMI - Université Paris Cité\\
\small IFLCE085 - Recherche et extraction sémantique
}

\date{}

\begin{document}

\maketitle



\section{Introduction}
Ce projet a pour dessein l'extraction de connaissances structurées à partir d'un corpus de textes non structurés. La première étape, cruciale, est le \textbf{preprocessing}, qui transforme les données brutes en un format exploitable.
Dans ce rapport, je présenterai le corpus et la méthodologie de preprocessing mise en œuvre. J'exposerai ensuite l'analyse statistique détaillée, incluant la distribution des \textbf{Part-of-Speech (POS)} et la vectorisation \textbf{TF-IDF}. Pour conclure, je justifierai mes choix techniques et leurs impacts sur la suite du projet.

\subsection{Le Corpus}
Le dataset (\texttt{data.csv}) est constitué de \textbf{2221 phrases} annotées pour la reconnaissance d'entités nommées (NER).
\begin{itemize}
    \item \textbf{Format d'entrée} : CSV avec colonnes \texttt{id}, \texttt{words}, \texttt{ner\_tags}, \texttt{text}.
    \item \textbf{Contenu} : Textes encyclopédiques et biographiques.
    \item \textbf{Annotations} : Tags BIO pour les entités Personnes, Lieux, Organisations, etc.
\end{itemize}

\section{Méthodologie de Preprocessing}

Contrairement à une approche classique de "Bag of Words", j'ai opté pour un \textbf{preprocessing enrichi} préservant la structure séquentielle, indispensable pour le NER.

\subsection{Pipeline Mis en Place}
J'ai développé le traitement en Python via un notebook Jupyter, selon les étapes suivantes :

\begin{enumerate}
    \item \textbf{Nettoyage (\texttt{cleaned\_text})} :
    \begin{itemize}
        \item Conversion en minuscules pour réduire la dimensionnalité.
        \item Suppression des caractères spéciaux non alphanumériques.
        \item Normalisation des espaces.
    \end{itemize}
    
    \item \textbf{Lemmatization (\texttt{lemmatized\_text})} :
    \begin{itemize}
        \item Utilisation de la librairie \textbf{spaCy}.
        \item Transformation des mots en leur forme canonique.
    \end{itemize}
    
    \item \textbf{Enrichissement du Dataset} :
    \begin{itemize}
        \item J'ai ajouté les colonnes \texttt{cleaned\_text} et \texttt{lemmatized\_text} au fichier original.
        \item \textbf{Sortie} : \texttt{data\_preprocessed.csv}.
    \end{itemize}
\end{enumerate}

\section{Analyse Statistique du Corpus}

L'analyse que j'ai menée dans le notebook \texttt{PartieA\_Analyse\_Statistique.ipynb} a révélé les caractéristiques suivantes :

\subsection{Volumétrie et Longueur}
\begin{itemize}
    \item \textbf{Nombre de phrases} : 2221.
    \item \textbf{Longueur moyenne} : $\sim$24 mots par phrase.
    \item \textbf{Impact du preprocessing} : Comme illustré par la Figure \ref{fig:impact}, la lemmatization réduit significativement la taille du vocabulaire (d'environ \textbf{15\% à 25\%}). Cela permet de "densifier" l'information en regroupant les variantes d'un même mot.
    \item \textbf{Réduction des outliers} : On observe sur la Figure \ref{fig:impact} une diminution drastique de la longueur des phrases "outliers" (points situés au-dessus des moustaches). Cela s'explique par le nettoyage qui supprime les caractères spéciaux, la ponctuation excessive et les métadonnées parasites souvent responsables de ces longueurs aberrantes dans le texte brut.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{stats_distributions.png}
    \caption{Distribution des longueurs de phrases et de mots}
    \label{fig:dist}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{stats_preprocessing_impact.png}
    \caption{Impact du preprocessing sur la taille du vocabulaire et la longueur des textes}
    \label{fig:impact}
\end{figure}

\subsection{Analyse Lexicale}
L'analyse des mots les plus fréquents et du nuage de mots met en évidence les thématiques dominantes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{stats_top_words.png}
    \caption{Mots les plus fréquents (hors stop-words)}
    \label{fig:topwords}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{stats_wordcloud.png}
    \caption{Nuage de mots du corpus}
    \label{fig:wordcloud}
\end{figure}

\subsection{Analyse POS (Part-of-Speech)}
\textbf{Choix de la méthode} : Utilisation du \textit{tagger} probabiliste de la librairie \textbf{spaCy} (\texttt{en\_core\_web\_sm}).

\textbf{Avantages} :
\begin{itemize}
    \item Rapidité d'exécution et robustesse sur l'anglais standard.
    \item Fournit des étiquettes universelles (UPOS) faciles à interpréter.
\end{itemize}

\textbf{Inconvénients} :
\begin{itemize}
    \item Sensible aux erreurs de capitalisation (le passage en minuscules peut créer des ambiguïtés, ex: "Apple" vs "apple").
\end{itemize}

Sur 17 151 tokens, la distribution est la suivante :



Sur 17 151 tokens, la distribution est présentée en Figure \ref{fig:pos}. On note une prédominance des noms et noms propres (30.5\%), typique de textes biographiques.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{pos_distribution.png}
    \caption{Distribution des catégories POS (Top 15)}
    \label{fig:pos}
\end{figure}

\subsection{Vectorisation TF-IDF}
\textbf{Choix de la méthode} : Vectorisation \textbf{TF-IDF} (Term Frequency-Inverse Document Frequency).

\textbf{Avantages} :
\begin{itemize}
    \item Permet de pondérer les termes selon leur importance relative (pénalise les mots très fréquents et peu informatifs comme "the", "is").
    \item Réduit l'impact des "stop words" sans les supprimer brutalement.
\end{itemize}

\textbf{Inconvénients} :
\begin{itemize}
    \item Perte de la sémantique séquentielle (approche "Bag of Words").
    \item Génère des matrices très creuses (haute dimensionnalité).
\end{itemize}

\textbf{Exemple d'application du corpus :}
Considérons la phrase suivante issue du dataset (en-doc6361-sent4) :
\textit{"The study of logic led directly to the invention of the programmable digital electronic computer, based on the work of mathematician Alan Turing and others."}

Après prétraitement (nettoyage, lemmatisation), les termes comme \textit{"programmable"}, \textit{"digital"}, \textit{"electronic"} et \textit{"computer"} reçoivent des scores TF-IDF élevés car ils sont spécifiques à ce document par rapport au reste du corpus, tandis que des mots comme \textit{"the"} ou \textit{"of"} ont un score proche de zéro.

J'ai obtenu une matrice de \textbf{700 documents $\times$ 2493 features}.

\textbf{Caractéristiques :}
\begin{itemize}
    \item \textbf{Vocabulaire} : 2493 termes uniques.
    \item \textbf{Densité} : 0.77\% (La matrice est très creuse, signifiant que chaque document ne contient qu'une infime fraction du vocabulaire global, ce qui est typique des données textuelles).
    \item \textbf{Paramètres} :
    \begin{itemize}
        \item \texttt{max\_df=0.8} : J'ai exclu les termes présents dans plus de 80\% des documents. Ces mots sont considérés comme des "stop words" spécifiques au corpus (trop fréquents pour être discriminants).
        \item \texttt{min\_df=2} : J'ai ignoré les termes apparaissant dans moins de 2 documents. Cela permet d'éliminer le bruit (fautes de frappe, hapax legomena) et de réduire la dimensionnalité de la matrice sans perdre d'information généralisable.
        \item \texttt{ngram\_range=(1,2)} : J'ai inclus non seulement les mots uniques (unigrammes) mais aussi les paires de mots consécutifs (bigrammes). Cela permet de capturer des concepts composés (ex: "machine learning", "new york") qui ont un sens propre différent de la somme de leurs parties.
    \end{itemize}
\end{itemize}

La figure \ref{fig:tfidf} présente les termes aux scores TF-IDF les plus élevés.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{tfidf_top_terms.png}
    \caption{Top 20 termes par score TF-IDF moyen}
    \label{fig:tfidf}
\end{figure}

\textbf{Interprétation} : La Figure \ref{fig:tfidf} montre que les termes ayant les scores TF-IDF les plus élevés restent des mots très courants de la langue anglaise ("the", "be", "of", "in"). Cela indique que ces termes sont présents dans une grande majorité des documents avec une fréquence élevée. Bien que le TF-IDF pénalise théoriquement les mots présents partout (IDF faible), leur fréquence brute (TF) est ici si importante qu'elle compense cette pénalité. Pour des tâches ultérieures plus fines, il serait pertinent d'envisager une liste de "stop words" plus agressive pour faire ressortir des termes plus spécifiques au domaine.


\section{Analyse Critique des Choix}

\subsection{Choix 1 : Lemmatization vs Stemming}
\begin{itemize}
    \item \textbf{Choix} : Lemmatization (spaCy).
    \item \textbf{Avantages} : Préserve le sens sémantique, indispensable pour l'extraction de relations.
    \item \textbf{Inconvénients} : Coûteux en calcul, perte de certaines nuances flexionnelles.
\end{itemize}

\subsection{Choix 2 : Lowercasing}
\begin{itemize}
    \item \textbf{Choix} : Tout mettre en minuscule.
    \item \textbf{Avantages} : Réduit la taille du vocabulaire.
    \item \textbf{Inconvénients} : \textbf{Perte de la capitalisation}, indice crucial pour le NER.
    \item \textbf{Mitigation} : J'ai conservé la colonne \texttt{text} originale pour pallier ce défaut.
\end{itemize}

\subsection{Choix 3 : Conservation de la Structure Séquentielle}
\begin{itemize}
    \item \textbf{Choix} : Pas de vectorisation TF-IDF pour l'export final.
    \item \textbf{Avantages} : Indispensable pour le NER et l'extraction de relations.
    \item \textbf{Inconvénients} : Fichiers plus volumineux.
\end{itemize}

\section{Impact sur la Suite du Projet}

\subsection{Impact sur la Partie II (NER)}
Le preprocessing fournit une base propre. Toutefois, pour maximiser les résultats, il me faudra utiliser :
\begin{enumerate}
    \item Le \textbf{texte original} pour la capitalisation.
    \item Le \textbf{texte lemmatisé} pour les embeddings sémantiques.
\end{enumerate}

\subsection{Impact sur la Partie C (Extraction de Relations)}
L'extraction de relations bénéficie de la lemmatization qui normalise les verbes et simplifie les arbres de dépendance.

\section{Conclusion}
En conclusion, je ne ferai qu'insister sur le fait que le preprocessing réalisé est un compromis entre réduction de bruit et préservation de l'information. Ce socle de données enrichi est robuste et adapté aux défis que j'aurai à relever par la suite : la reconnaissance fine d'entités et l'extraction de relations.

\end{document}
