\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}

\geometry{margin=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Rapport Final Partie A - Preprocessing},
    pdfauthor={Jacques Gastebois},
}

% Configuration du code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    language=Python
}

% En-tête et pied de page
\pagestyle{fancy}
\fancyhf{}
\rhead{Partie A : Preprocessing \& Analyse}
\lhead{Projet Sémantique}
\cfoot{\thepage}

\title{\textbf{Projet Knowledge Extraction}\\
\large Partie A : Preprocessing, Analyse Statistique et Justification des Choix\\
\vspace{0.5cm}
\normalsize Rapport Technique Final}

\author{
Jacques Gastebois\\
\small Master 2 VMI - Université Paris Cité\\
\small IFLCE085 - Recherche et extraction sémantique
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport relate les travaux que j'ai réalisés pour la Partie A du projet. Il a pour dessein de présenter le pipeline de preprocessing appliqué au corpus NER, d'en exposer l'analyse statistique et de justifier mes choix techniques. J'y aborde les avantages et inconvénients de ces traitements ainsi que leurs impacts sur les tâches d'extraction d'entités et de relations.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
Ce projet a pour dessein l'extraction de connaissances structurées à partir d'un corpus de textes non structurés. La première étape, cruciale, est le \textbf{preprocessing}, qui transforme les données brutes en un format exploitable.
Dans ce rapport, je commencerai par la présentation du corpus. Par la suite, je décrirai la méthodologie que j'ai mise en œuvre. Pour conclure, je m'arrêterai sur l'analyse critique de mes choix et leurs impacts sur la suite du projet.

\subsection{Le Corpus}
Le dataset (\texttt{data.csv}) est constitué de \textbf{2221 phrases} annotées pour la reconnaissance d'entités nommées (NER).
\begin{itemize}
    \item \textbf{Format d'entrée} : CSV avec colonnes \texttt{id}, \texttt{words}, \texttt{ner\_tags}, \texttt{text}.
    \item \textbf{Contenu} : Textes encyclopédiques et biographiques.
    \item \textbf{Annotations} : Tags BIO pour les entités Personnes, Lieux, Organisations, etc.
\end{itemize}

\section{Méthodologie de Preprocessing}

Contrairement à une approche classique de "Bag of Words", j'ai opté pour un \textbf{preprocessing enrichi} préservant la structure séquentielle, indispensable pour le NER.

\subsection{Pipeline Mis en Place}
J'ai développé le traitement en Python via un notebook Jupyter, selon les étapes suivantes :

\begin{enumerate}
    \item \textbf{Nettoyage (\texttt{cleaned\_text})} :
    \begin{itemize}
        \item Conversion en minuscules pour réduire la dimensionnalité.
        \item Suppression des caractères spéciaux non alphanumériques.
        \item Normalisation des espaces.
    \end{itemize}
    
    \item \textbf{Lemmatization (\texttt{lemmatized\_text})} :
    \begin{itemize}
        \item Utilisation de la librairie \textbf{spaCy}.
        \item Transformation des mots en leur forme canonique.
    \end{itemize}
    
    \item \textbf{Enrichissement du Dataset} :
    \begin{itemize}
        \item J'ai ajouté les colonnes \texttt{cleaned\_text} et \texttt{lemmatized\_text} au fichier original.
        \item \textbf{Sortie} : \texttt{data\_preprocessed.csv}.
    \end{itemize}
\end{enumerate}

\section{Analyse Statistique du Corpus}

L'analyse que j'ai menée dans le notebook \texttt{PartieA\_Analyse\_Statistique.ipynb} a révélé les caractéristiques suivantes :

\subsection{Volumétrie et Longueur}
\begin{itemize}
    \item \textbf{Nombre de phrases} : 2221.
    \item \textbf{Longueur moyenne} : $\sim$24 mots par phrase.
    \item \textbf{Impact du preprocessing} : La lemmatization réduit la taille du vocabulaire d'environ \textbf{15\% à 25\%}, densifiant l'information sans perte de sens.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{stats_distributions.png}
    \caption{Distribution des longueurs de phrases et de mots}
    \label{fig:dist}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{stats_preprocessing_impact.png}
    \caption{Impact du preprocessing sur la taille du vocabulaire}
    \label{fig:impact}
\end{figure}

\subsection{Analyse Lexicale}
L'analyse des mots les plus fréquents et du nuage de mots met en évidence les thématiques dominantes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{stats_top_words.png}
    \caption{Mots les plus fréquents (hors stop-words)}
    \label{fig:topwords}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{stats_wordcloud.png}
    \caption{Nuage de mots du corpus}
    \label{fig:wordcloud}
\end{figure}

\subsection{Analyse POS (Part-of-Speech)}
J'ai analysé le corpus avec spaCy pour identifier les catégories grammaticales. Sur 17 151 tokens, la distribution est la suivante :

\begin{table}[h]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{POS Tag} & \textbf{Count} & \textbf{Pourcentage} \\ \midrule
NOUN (Nom) & 2994 & 17.46\% \\
PROPN (Nom propre) & 2241 & 13.07\% \\
ADP (Préposition) & 2104 & 12.27\% \\
PUNCT (Ponctuation) & 2049 & 11.95\% \\
DET (Déterminant) & 1562 & 9.11\% \\ \bottomrule
\end{tabular}
\caption{Top 5 des catégories POS}
\end{table}

La figure \ref{fig:pos} montre la prédominance des noms et noms propres (30.5\%), typique de textes biographiques.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{pos_distribution.png}
    \caption{Distribution des catégories POS (Top 15)}
    \label{fig:pos}
\end{figure}

\subsection{Vectorisation TF-IDF}
J'ai transformé le corpus lemmatisé en représentation TF-IDF, obtenant une matrice de \textbf{700 documents × 2493 features}.

\textbf{Caractéristiques :}
\begin{itemize}
    \item \textbf{Vocabulaire} : 2493 termes uniques.
    \item \textbf{Sparsité} : 0.77\%.
    \item \textbf{Paramètres} : max\_df=0.8, min\_df=2, ngram\_range=(1,2).
\end{itemize}

La figure \ref{fig:tfidf} présente les termes aux scores TF-IDF les plus élevés.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{tfidf_top_terms.png}
    \caption{Top 20 termes par score TF-IDF moyen}
    \label{fig:tfidf}
\end{figure}


\section{Analyse Critique des Choix}

\subsection{Choix 1 : Lemmatization vs Stemming}
\begin{itemize}
    \item \textbf{Choix} : Lemmatization (spaCy).
    \item \textbf{Avantages} : Préserve le sens sémantique, indispensable pour l'extraction de relations.
    \item \textbf{Inconvénients} : Coûteux en calcul, perte de certaines nuances flexionnelles.
\end{itemize}

\subsection{Choix 2 : Lowercasing}
\begin{itemize}
    \item \textbf{Choix} : Tout mettre en minuscule.
    \item \textbf{Avantages} : Réduit la taille du vocabulaire.
    \item \textbf{Inconvénients} : \textbf{Perte de la capitalisation}, indice crucial pour le NER.
    \item \textbf{Mitigation} : J'ai conservé la colonne \texttt{text} originale pour pallier ce défaut.
\end{itemize}

\subsection{Choix 3 : Conservation de la Structure Séquentielle}
\begin{itemize}
    \item \textbf{Choix} : Pas de vectorisation TF-IDF pour l'export final.
    \item \textbf{Avantages} : Indispensable pour le NER et l'extraction de relations.
    \item \textbf{Inconvénients} : Fichiers plus volumineux.
\end{itemize}

\section{Impact sur la Suite du Projet}

\subsection{Impact sur la Partie II (NER)}
Le preprocessing fournit une base propre. Toutefois, pour maximiser les résultats, il me faudra utiliser :
\begin{enumerate}
    \item Le \textbf{texte original} pour la capitalisation.
    \item Le \textbf{texte lemmatisé} pour les embeddings sémantiques.
\end{enumerate}

\subsection{Impact sur la Partie C (Extraction de Relations)}
L'extraction de relations bénéficie de la lemmatization qui normalise les verbes et simplifie les arbres de dépendance.

\section{Conclusion}
En conclusion, je ne ferai qu'insister sur le fait que le preprocessing réalisé est un compromis entre réduction de bruit et préservation de l'information. Ce socle de données enrichi est robuste et adapté aux défis que j'aurai à relever par la suite : la reconnaissance fine d'entités et l'extraction de relations.

\end{document}
